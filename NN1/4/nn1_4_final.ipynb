{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:09.712263Z",
     "start_time": "2024-03-26T07:59:09.709266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikolajmroz/Developer/Computational_Intelligence_Methods\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/mikolajmroz/Developer/Computational_Intelligence_Methods')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:09.869603Z",
     "start_time": "2024-03-26T07:59:09.867825Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:10.015481Z",
     "start_time": "2024-03-26T07:59:10.013781Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:10.184929Z",
     "start_time": "2024-03-26T07:59:10.183015Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:11.754922Z",
     "start_time": "2024-03-26T07:59:11.752458Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:11.909437Z",
     "start_time": "2024-03-26T07:59:11.907317Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # Corrected sigmoid derivative to properly compute the derivative\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:12.050982Z",
     "start_time": "2024-03-26T07:59:12.049192Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:12.353939Z",
     "start_time": "2024-03-26T07:59:12.351777Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:12.503212Z",
     "start_time": "2024-03-26T07:59:12.501014Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_output, y_true):\n",
    "    # Assuming y_true is one-hot encoded\n",
    "    m = y_true.shape[1]  # Number of examples\n",
    "    log_likelihood = -np.log(softmax_output[y_true.argmax(axis=0), range(m)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:12.659947Z",
     "start_time": "2024-03-26T07:59:12.658096Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_derivative(softmax_output, y_true):\n",
    "\n",
    "    corrected_softmax_output = softmax_output - y_true\n",
    "    \n",
    "    return corrected_softmax_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:59:12.808400Z",
     "start_time": "2024-03-26T07:59:12.806748Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_derivative(softmax_output):\n",
    "    # For softmax combined with cross-entropy loss, the derivative simplifies\n",
    "    # the gradient calculation in backpropagation, directly using output error.\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:21:50.565901Z",
     "start_time": "2024-03-26T09:21:50.469870Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, sizes, activation_fn=relu, activation_fn_derivative=relu_derivative):\n",
    "        self.layer_sizes = sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.layer_weights = [np.random.randn(y, x) * np.sqrt(2. / x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.layer_biases = [np.zeros((y, 1)) for y in sizes[1:]]\n",
    "        self.activation_fn_derivative = activation_fn_derivative\n",
    "\n",
    "    def display_weights_biases(self):\n",
    "        print(\"Final Weights and Biases:\")\n",
    "        for layer_index, (weights, biases) in enumerate(zip(self.layer_weights, self.layer_biases)):\n",
    "            print(f\"Layer {layer_index + 1} Weights:\\n{weights}\")\n",
    "            print(f\"Layer {layer_index + 1} Biases:\\n{biases}\")\n",
    "\n",
    "    def propagate_forward(self, input_activation):\n",
    "        activations = [input_activation]\n",
    "        for biases, weights in zip(self.layer_biases, self.layer_weights[:-1]):\n",
    "            input_activation = self.activation_fn(np.dot(weights, input_activation) + biases)\n",
    "            activations.append(input_activation)\n",
    "        final_input = np.dot(self.layer_weights[-1], input_activation) + self.layer_biases[-1]\n",
    "        output_activation = softmax(final_input)\n",
    "        activations.append(output_activation)\n",
    "        # change\n",
    "        return output_activation, activations\n",
    "\n",
    "    def backward_propagation(self, input_val, true_val):\n",
    "        weight_gradients = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        bias_gradients = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        final_act, activations = self.propagate_forward(input_val)\n",
    "        error = cross_entropy_derivative(final_act, true_val)\n",
    "        # change\n",
    "        bias_gradients[-1] = error\n",
    "        weight_gradients[-1] = np.dot(error, activations[-2].T)\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = np.dot(self.layer_weights[-l+1].T, error)\n",
    "            error = self.activation_fn_derivative(z) * z\n",
    "            bias_gradients[-l] = error\n",
    "            weight_gradients[-l] = np.dot(error, activations[-l-1].T)\n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def update_batch(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon=1e-8):\n",
    "        gradient_w = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        gradient_b = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        \n",
    "        for input_val, true_val in batch:\n",
    "            delta_gradient_w, delta_gradient_b = self.backward_propagation(input_val, true_val)\n",
    "            gradient_w = [w + dw for w, dw in zip(gradient_w, delta_gradient_w)]\n",
    "            gradient_b = [b + db for b, db in zip(gradient_b, delta_gradient_b)]\n",
    "\n",
    "        # Update rule for weights and biases based on the optimization method\n",
    "        if optimization_method == 'momentum':\n",
    "            # Momentum initialization\n",
    "            if not hasattr(self, 'velocity_weights'):\n",
    "                self.velocity_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.velocity_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "\n",
    "            # Update velocities\n",
    "            self.velocity_weights = [beta * vw + (1 - beta) * gw / len(batch) for vw, gw in zip(self.velocity_weights, gradient_w)]\n",
    "            self.velocity_biases = [beta * vb + (1 - beta) * gb / len(batch) for vb, gb in zip(self.velocity_biases, gradient_b)]\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - learn_rate * vw\n",
    "                                  for w, vw in zip(self.layer_weights, self.velocity_weights)]\n",
    "            self.layer_biases = [b - learn_rate * vb for b, vb in zip(self.layer_biases, self.velocity_biases)]\n",
    "        elif optimization_method == 'rmsprop':\n",
    "            # RMSprop initialization\n",
    "            if not hasattr(self, 'squared_gradients_weights'):\n",
    "                self.squared_gradients_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.squared_gradients_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "\n",
    "            # Update squared gradients\n",
    "            self.squared_gradients_weights = [beta * sgw + (1 - beta) * (gw**2) / len(batch)\n",
    "                                              for sgw, gw in zip(self.squared_gradients_weights, gradient_w)]\n",
    "            self.squared_gradients_biases = [beta * sgb + (1 - beta) * (gb**2) / len(batch)\n",
    "                                             for sgb, gb in zip(self.squared_gradients_biases, gradient_b)]\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - \n",
    "                                  learn_rate * gw / (np.sqrt(sgw) + epsilon)\n",
    "                                  for w, sgw, gw in zip(self.layer_weights, self.squared_gradients_weights, gradient_w)]\n",
    "            self.layer_biases = [b - learn_rate * gb / (np.sqrt(sgb) + epsilon)\n",
    "                                 for b, sgb, gb in zip(self.layer_biases, self.squared_gradients_biases, gradient_b)]\n",
    "\n",
    "    def train(self, training_data, epochs, learn_rate, batch_size, regularization=0.0, optimization_method='rmsprop', beta=0.9, epsilon=1e-8, visual_interval=10, X_val=None, y_val=None, target = None,adaptive_learn_rate = True, decay_rate=0.1, decay_step=100):\n",
    "        n = len(training_data)\n",
    "        \n",
    "        # Determine mini-batch size based on whether the batch_size_input is a percentage or fixed value\n",
    "        if isinstance(batch_size, float):  # If batch_size_input is a float, treat it as a percentage\n",
    "            batch_size = max(1, min(n, int(n * batch_size / 100)))\n",
    "        elif isinstance(batch_size, int):  # If batch_size_input is an integer, treat it as a fixed size\n",
    "            batch_size = max(1, min(n, batch_size))\n",
    "        else:  # Raise an error if batch_size_input is neither float nor int\n",
    "            raise ValueError(\"batch_size_input must be an integer (fixed size) or a float (percentage of dataset)\")\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k + batch_size] for k in range(0, n, batch_size)]\n",
    "    \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_batch(mini_batch, learn_rate, regularization, n, optimization_method, beta, epsilon)\n",
    "            if adaptive_learn_rate:\n",
    "                # Decay the learning rate every decay_step epochs\n",
    "                if epoch % decay_step == 0 and epoch > 0:\n",
    "                    learn_rate *= (1. / (1. + decay_rate * epoch))\n",
    "    \n",
    "            if epoch % visual_interval == 0:\n",
    "                predictions = np.argmax(np.array([self.propagate_forward(x.reshape(-1, 1))[0] for x in X_val]), axis=1)\n",
    "                accuracy = np.mean(predictions == y_val)\n",
    "                print(f'epoch: {epoch}', f'Test accuracy: {accuracy}')\n",
    "                f1_weighted = f1_score(y_val, predictions, average='weighted')\n",
    "                print(f\"F1 Score (Weighted): {f1_weighted}\")\n",
    "                \n",
    "                if f1_weighted > target:\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T08:03:24.693200Z",
     "start_time": "2024-03-26T08:03:24.689403Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataScaler:\n",
    "    def __init__(self, method=\"standardization\"):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.fit_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.fit_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.inverse_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.inverse_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def fit_transform_min_max(self, data):\n",
    "        self.min = np.min(data, axis=0)\n",
    "        self.max = np.max(data, axis=0)\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def transform_min_max(self, data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse_transform_min_max(self, data):\n",
    "        return data * (self.max - self.min) + self.min\n",
    "\n",
    "    def fit_transform_standardization(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def transform_standardization(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform_standardization(self, data):\n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T08:03:24.845144Z",
     "start_time": "2024-03-26T08:03:24.843342Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse(mse_history):\n",
    "    plt.plot(mse_history)\n",
    "    plt.title('MSE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T08:03:25.149523Z",
     "start_time": "2024-03-26T08:03:25.144424Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_rings3_regular = pd.read_csv('./data/classification/rings3-regular-training.csv')\n",
    "df_test_rings3_regular = pd.read_csv('./data/classification/rings3-regular-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T08:03:25.308621Z",
     "start_time": "2024-03-26T08:03:25.304810Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_easy = pd.read_csv('./data/classification/easy-training.csv')\n",
    "df_test_easy = pd.read_csv('./data/classification/easy-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T08:03:25.471229Z",
     "start_time": "2024-03-26T08:03:25.467690Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_xor3 = pd.read_csv('./data/classification/xor3-training.csv')\n",
    "df_test_xor3 = pd.read_csv('./data/classification/xor3-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rings 3 regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:02.202295Z",
     "start_time": "2024-03-26T09:22:02.200009Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler_X = DataScaler(\"standardization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:02.848366Z",
     "start_time": "2024-03-26T09:22:02.845980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_rings = df_train_rings3_regular[['x']].values.reshape(-1, 1)\n",
    "X1_test_rings = df_test_rings3_regular[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:04.265289Z",
     "start_time": "2024-03-26T09:22:04.262430Z"
    }
   },
   "outputs": [],
   "source": [
    "X2_train_rings = df_train_rings3_regular[['y']].values.reshape(-1, 1)\n",
    "X2_test_rings = df_test_rings3_regular[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:04.800687Z",
     "start_time": "2024-03-26T09:22:04.798706Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rings = np.hstack((X1_train_rings, X2_train_rings))\n",
    "X_test_rings = np.hstack((X1_test_rings, X2_test_rings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:07.159363Z",
     "start_time": "2024-03-26T09:22:07.156181Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rings_scaled = np.hstack((scaler_X.fit_transform(X1_train_rings), scaler_X.fit_transform(X2_train_rings)))\n",
    "X_test_rings_scaled = np.hstack((scaler_X.transform(X1_test_rings), scaler_X.transform(X2_test_rings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:07.747767Z",
     "start_time": "2024-03-26T09:22:07.745735Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_rings = df_train_rings3_regular['c'].values.reshape(-1, 1)\n",
    "y_test_rings = df_test_rings3_regular['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:11.811661Z",
     "start_time": "2024-03-26T09:22:11.807642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_rings = encoder.fit_transform(y_train_rings)\n",
    "y_test_encoded_rings = encoder.transform(y_test_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:12.384695Z",
     "start_time": "2024-03-26T09:22:12.382447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_rings = y_train_encoded_rings.shape[1] \n",
    "num_classes_rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:22:16.063821Z",
     "start_time": "2024-03-26T09:22:16.059736Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_rings = [\n",
    "    (X_train_rings[i].reshape(-1, 1), y_train_encoded_rings[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_rings))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "mlp_rings = MLP(sizes=[2, 5, 5, 3], activation_fn = sigmoid, activation_fn_derivative = sigmoid_derivative)  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "\n",
    "mlp_rings.train(training_data=training_data_rings, epochs=10000, learn_rate=0.1, batch_size=24, X_val=X_test_rings, y_val=y_test_rings, visual_interval=10, target = 0.75, decay_rate=0.01, adaptive_learn_rate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:33:03.856931Z",
     "start_time": "2024-03-26T07:33:03.824725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_rings = np.argmax(np.array([mlp_rings.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_rings]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:33:04.189575Z",
     "start_time": "2024-03-26T07:33:04.187155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.628\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy or other metrics\n",
    "accuracy_rings = np.mean(predictions_rings == y_test_rings)\n",
    "print(f'Test accuracy: {accuracy_rings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T07:33:04.521413Z",
     "start_time": "2024-03-26T07:33:04.517164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.631703204250066\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted_rings = f1_score(y_test_rings, predictions_rings, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_rings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### easy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:02.246565Z",
     "start_time": "2024-03-26T09:09:02.237502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_easy = df_train_easy[['x']].values.reshape(-1, 1)\n",
    "X1_test_easy = df_test_easy[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:02.613609Z",
     "start_time": "2024-03-26T09:09:02.611434Z"
    }
   },
   "outputs": [],
   "source": [
    "X2_train_easy = df_train_easy[['y']].values.reshape(-1, 1)\n",
    "X2_test_easy = df_test_easy[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:02.839671Z",
     "start_time": "2024-03-26T09:09:02.837410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_easy= np.hstack((X1_train_easy, X2_train_easy))\n",
    "X_test_easy = np.hstack((X1_test_easy, X2_test_easy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:03.011877Z",
     "start_time": "2024-03-26T09:09:03.009657Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_easy = df_train_easy['c'].values.reshape(-1, 1)\n",
    "y_test_easy = df_test_easy['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:03.236173Z",
     "start_time": "2024-03-26T09:09:03.231618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_easy = encoder.fit_transform(y_train_easy)\n",
    "y_test_encoded_easy = encoder.transform(y_test_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:03.412783Z",
     "start_time": "2024-03-26T09:09:03.410118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_easy = y_train_encoded_easy.shape[1] \n",
    "num_classes_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:09:03.587059Z",
     "start_time": "2024-03-26T09:09:03.583490Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_easy = [\n",
    "    (X_train_easy[i].reshape(-1, 1), y_train_encoded_easy[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_easy))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:10:26.135495Z",
     "start_time": "2024-03-26T09:10:25.789741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.792\n",
      "F1 Score (Weighted): 0.7850488078726637\n",
      "epoch: 10 Test accuracy: 0.952\n",
      "F1 Score (Weighted): 0.9519631147540983\n",
      "epoch: 20 Test accuracy: 0.99\n",
      "F1 Score (Weighted): 0.9899981994166109\n",
      "epoch: 30 Test accuracy: 0.99\n",
      "F1 Score (Weighted): 0.9899981994166109\n",
      "epoch: 40 Test accuracy: 0.992\n",
      "F1 Score (Weighted): 0.991998975737789\n"
     ]
    }
   ],
   "source": [
    "mlp_easy = MLP(sizes=[2, 2, 2])  \n",
    "\n",
    "# Train the MLP using your training data\n",
    "mlp_easy.train(training_data=training_data_easy, epochs=100, learn_rate=0.01, batch_size=20, X_val=X_test_easy, y_val=y_test_easy, visual_interval=10, target = 0.99, decay_rate=0.001, adaptive_learn_rate=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.914\n",
      "F1 Score (Weighted): 0.9139845149828545\n",
      "epoch: 10 Test accuracy: 0.956\n",
      "F1 Score (Weighted): 0.9559302135567695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 Test accuracy: 0.962\n",
      "F1 Score (Weighted): 0.9619566409768793\n",
      "epoch: 30 Test accuracy: 0.97\n",
      "F1 Score (Weighted): 0.9699801904121593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 Test accuracy: 0.968\n",
      "F1 Score (Weighted): 0.9679754098360657\n",
      "epoch: 50 Test accuracy: 0.97\n",
      "F1 Score (Weighted): 0.97000036000144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60 Test accuracy: 0.986\n",
      "F1 Score (Weighted): 0.9859956859119814\n",
      "epoch: 70 Test accuracy: 0.98\n",
      "F1 Score (Weighted): 0.9799887912123104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80 Test accuracy: 0.978\n",
      "F1 Score (Weighted): 0.9779932226716437\n",
      "epoch: 90 Test accuracy: 0.988\n",
      "F1 Score (Weighted): 0.987997118847539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/kcmcwh_95_jgg3sg1zp_pp5m0000gn/T/ipykernel_82452/1956533664.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "mlp_easy2 = MLP(sizes=[2, 2, 2], activation_fn=sigmoid, activation_fn_derivative=sigmoid_derivative)  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "mlp_easy2.train(training_data=training_data_easy, epochs=100, learn_rate=0.01, batch_size=20, X_val=X_test_easy, y_val=y_test_easy, visual_interval=10, target = 0.99, decay_rate=0.001, adaptive_learn_rate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:17:51.011963Z",
     "start_time": "2024-03-26T09:17:49.938287Z"
    }
   },
   "execution_count": 391
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_easy = np.argmax(np.array([mlp_easy.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_easy]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:10:40.579966Z",
     "start_time": "2024-03-26T09:10:40.572342Z"
    }
   },
   "execution_count": 383
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:10:41.075077Z",
     "start_time": "2024-03-26T09:10:41.073118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_easy = np.mean(predictions_easy == y_test_easy)\n",
    "print(f'Test accuracy: {accuracy_easy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T09:10:45.315292Z",
     "start_time": "2024-03-26T09:10:45.311349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.991998975737789\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted_easy = f1_score(y_test_easy, predictions_easy, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_easy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_xor3 = df_train_xor3[['x']].values.reshape(-1, 1)\n",
    "X1_test_xor3 = df_test_xor3[['x']].values.reshape(-1, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:42:59.344439Z",
     "start_time": "2024-03-26T00:42:59.342282Z"
    }
   },
   "execution_count": 248
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X2_train_xor3 = df_train_xor3[['y']].values.reshape(-1, 1)\n",
    "X2_test_xor3 = df_test_xor3[['y']].values.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:42:59.572169Z",
     "start_time": "2024-03-26T00:42:59.569608Z"
    }
   },
   "execution_count": 249
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_xor3= np.hstack((X1_train_xor3, X2_train_xor3))\n",
    "X_test_xor3 = np.hstack((X1_test_xor3, X2_test_xor3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:42:59.766540Z",
     "start_time": "2024-03-26T00:42:59.764468Z"
    }
   },
   "execution_count": 250
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train_xor3 = df_train_xor3['c'].values.reshape(-1, 1)\n",
    "y_test_xor3 = df_test_xor3['c'].values.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:42:59.979560Z",
     "start_time": "2024-03-26T00:42:59.977245Z"
    }
   },
   "execution_count": 251
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_xor3 = encoder.fit_transform(y_train_xor3)\n",
    "y_test_encoded_xor3 = encoder.transform(y_test_xor3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:00.163051Z",
     "start_time": "2024-03-26T00:43:00.159943Z"
    }
   },
   "execution_count": 252
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_xor3 = y_train_encoded_xor3.shape[1] \n",
    "num_classes_xor3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:00.837739Z",
     "start_time": "2024-03-26T00:43:00.835644Z"
    }
   },
   "execution_count": 253
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data_xor3 = [\n",
    "    (X_train_xor3[i].reshape(-1, 1), y_train_encoded_xor3[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_xor3))\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:01.543972Z",
     "start_time": "2024-03-26T00:43:01.541273Z"
    }
   },
   "execution_count": 254
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.548\n",
      "F1 Score (Weighted): 0.5476745547928767\n",
      "epoch: 100 Test accuracy: 0.584\n",
      "F1 Score (Weighted): 0.48200701754385966\n",
      "epoch: 200 Test accuracy: 0.56\n",
      "F1 Score (Weighted): 0.44939588433798716\n",
      "epoch: 300 Test accuracy: 0.642\n",
      "F1 Score (Weighted): 0.6044149259264541\n",
      "epoch: 400 Test accuracy: 0.68\n",
      "F1 Score (Weighted): 0.6613734692776609\n",
      "epoch: 500 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 600 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 700 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 800 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 900 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 1000 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 1100 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 1200 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 1300 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n",
      "epoch: 1400 Test accuracy: 0.686\n",
      "F1 Score (Weighted): 0.670225873201643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[255], line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m mlp_xor3 \u001B[38;5;241m=\u001B[39m MLP(sizes\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m2\u001B[39m])  \u001B[38;5;66;03m# Example layer setup\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Train the MLP using your training data\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# This step will vary depending on the exact implementation of your `train` method\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# For example:\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mmlp_xor3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_data_xor3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_test_xor3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_test_xor3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisual_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.97\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[201], line 89\u001B[0m, in \u001B[0;36mMLP.train\u001B[0;34m(self, training_data, epochs, learn_rate, batch_size, regularization, optimization_method, beta, epsilon, visual_interval, X_val, y_val, target, adaptive_learn_rate, decay_rate, decay_step)\u001B[0m\n\u001B[1;32m     86\u001B[0m mini_batches \u001B[38;5;241m=\u001B[39m [training_data[k:k \u001B[38;5;241m+\u001B[39m batch_size] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, n, batch_size)]\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[0;32m---> 89\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregularization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimization_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m adaptive_learn_rate:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;66;03m# Decay the learning rate every decay_step epochs\u001B[39;00m\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m decay_step \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[201], line 44\u001B[0m, in \u001B[0;36mMLP.update_batch\u001B[0;34m(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon)\u001B[0m\n\u001B[1;32m     41\u001B[0m gradient_b \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mzeros(bias\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m bias \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_biases]\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_val, true_val \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[0;32m---> 44\u001B[0m     delta_gradient_w, delta_gradient_b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward_propagation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrue_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     gradient_w \u001B[38;5;241m=\u001B[39m [w \u001B[38;5;241m+\u001B[39m dw \u001B[38;5;28;01mfor\u001B[39;00m w, dw \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(gradient_w, delta_gradient_w)]\n\u001B[1;32m     46\u001B[0m     gradient_b \u001B[38;5;241m=\u001B[39m [b \u001B[38;5;241m+\u001B[39m db \u001B[38;5;28;01mfor\u001B[39;00m b, db \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(gradient_b, delta_gradient_b)]\n",
      "Cell \u001B[0;32mIn[201], line 36\u001B[0m, in \u001B[0;36mMLP.backward_propagation\u001B[0;34m(self, input_val, true_val)\u001B[0m\n\u001B[1;32m     34\u001B[0m     error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_fn_derivative(z) \u001B[38;5;241m*\u001B[39m z\n\u001B[1;32m     35\u001B[0m     bias_gradients[\u001B[38;5;241m-\u001B[39ml] \u001B[38;5;241m=\u001B[39m error\n\u001B[0;32m---> 36\u001B[0m     weight_gradients[\u001B[38;5;241m-\u001B[39ml] \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivations\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43ml\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m weight_gradients, bias_gradients\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/multiarray.py:741\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(a, b, out)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;124;03m    result_type(*arrays_and_dtypes)\u001B[39;00m\n\u001B[1;32m    673\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    736\u001B[0m \n\u001B[1;32m    737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arrays_and_dtypes\n\u001B[0;32m--> 741\u001B[0m \u001B[38;5;129m@array_function_from_c_func_and_dispatcher\u001B[39m(_multiarray_umath\u001B[38;5;241m.\u001B[39mdot)\n\u001B[1;32m    742\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdot\u001B[39m(a, b, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    743\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    744\u001B[0m \u001B[38;5;124;03m    dot(a, b, out=None)\u001B[39;00m\n\u001B[1;32m    745\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    829\u001B[0m \n\u001B[1;32m    830\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    831\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a, b, out)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_xor3 = MLP(sizes=[2, 12, 2])  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_xor3.train(training_data=training_data_xor3, epochs=100000, learn_rate=0.01, batch_size=20, X_val=X_test_xor3, y_val=y_test_xor3, visual_interval=100, target = 0.97)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:24.935950Z",
     "start_time": "2024-03-26T00:43:14.114534Z"
    }
   },
   "execution_count": 255
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_xor3 = np.argmax(np.array([mlp_xor3.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_xor3]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:27.221717Z",
     "start_time": "2024-03-26T00:43:27.211766Z"
    }
   },
   "execution_count": 256
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_xor3 = np.mean(predictions_xor3 == y_test_xor3)\n",
    "print(f'Test accuracy: {accuracy_xor3}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:27.405136Z",
     "start_time": "2024-03-26T00:43:27.403088Z"
    }
   },
   "execution_count": 257
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.670225873201643\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted_xor3 = f1_score(y_test_xor3, predictions_xor3, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_xor3}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:43:27.611798Z",
     "start_time": "2024-03-26T00:43:27.608867Z"
    }
   },
   "execution_count": 258
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
