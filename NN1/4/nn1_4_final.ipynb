{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:18.137829Z",
     "start_time": "2024-04-02T11:47:18.135140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikolajmroz/Developer/Computational_Intelligence_Methods\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/mikolajmroz/Developer/Computational_Intelligence_Methods')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.854797Z",
     "start_time": "2024-04-02T11:47:18.138780Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.857958Z",
     "start_time": "2024-04-02T11:47:20.855881Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.860756Z",
     "start_time": "2024-04-02T11:47:20.859242Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.862926Z",
     "start_time": "2024-04-02T11:47:20.861239Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Avoid overflow\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.864986Z",
     "start_time": "2024-04-02T11:47:20.863460Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(sigmoid_output):\n",
    "    # Assumes that sigmoid_output is the result of sigmoid(x)\n",
    "    return sigmoid_output * (1 - sigmoid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.866999Z",
     "start_time": "2024-04-02T11:47:20.865392Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.868961Z",
     "start_time": "2024-04-02T11:47:20.867515Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.872092Z",
     "start_time": "2024-04-02T11:47:20.870481Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_output, y_true):\n",
    "    # Assuming y_true is one-hot encoded\n",
    "    m = y_true.shape[1]  # Number of examples\n",
    "    log_likelihood = -np.log(softmax_output[y_true.argmax(axis=0), range(m)] + 1e-9)  # Small constant added\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.873885Z",
     "start_time": "2024-04-02T11:47:20.872575Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_derivative(softmax_output, y_true):\n",
    "\n",
    "    corrected_softmax_output = softmax_output - y_true\n",
    "    \n",
    "    return corrected_softmax_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.875488Z",
     "start_time": "2024-04-02T11:47:20.874342Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_derivative(softmax_output):\n",
    "    # For softmax combined with cross-entropy loss, the derivative simplifies\n",
    "    # the gradient calculation in backpropagation, directly using output error.\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.886114Z",
     "start_time": "2024-04-02T11:47:20.876096Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, sizes, activation_fn=sigmoid, activation_fn_derivative=sigmoid_derivative):\n",
    "        self.layer_sizes = sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.layer_weights = [np.random.randn(y, x) * np.sqrt(2. / x) / 10 for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.layer_biases = [np.zeros((y, 1)) for y in sizes[1:]]\n",
    "        self.activation_fn_derivative = activation_fn_derivative\n",
    "\n",
    "    def display_weights_biases(self):\n",
    "        print(\"Final Weights and Biases:\")\n",
    "        for layer_index, (weights, biases) in enumerate(zip(self.layer_weights, self.layer_biases)):\n",
    "            print(f\"Layer {layer_index + 1} Weights:\\n{weights}\")\n",
    "            print(f\"Layer {layer_index + 1} Biases:\\n{biases}\")\n",
    "\n",
    "    def propagate_forward(self, input_activation):\n",
    "        activations = [input_activation]\n",
    "        for biases, weights in zip(self.layer_biases, self.layer_weights[:-1]):\n",
    "            input_activation = self.activation_fn(np.dot(weights, input_activation) + biases)\n",
    "            activations.append(input_activation)\n",
    "        final_input = np.dot(self.layer_weights[-1], input_activation) + self.layer_biases[-1]\n",
    "        output_activation = softmax(final_input)\n",
    "        activations.append(output_activation)\n",
    "        # change\n",
    "        return output_activation, activations\n",
    "\n",
    "    def backward_propagation(self, input_val, true_val):\n",
    "        weight_gradients = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        bias_gradients = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        \n",
    "        # Forward pass to get activations\n",
    "        final_act, activations = self.propagate_forward(input_val)\n",
    "        \n",
    "        # Start with the derivative of the loss function w.r.t. the final activation\n",
    "        error = cross_entropy_derivative(final_act, true_val)\n",
    "        \n",
    "        # Update gradients for the output layer\n",
    "        bias_gradients[-1] = error\n",
    "        weight_gradients[-1] = np.dot(error, activations[-2].T)\n",
    "        \n",
    "        # Backpropagate the error\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            # The derivative of the activation function is applied to the output of the activation function\n",
    "            # from the forward pass, hence 'activations[-l]'\n",
    "            activation_derivative = self.activation_fn_derivative(activations[-l])\n",
    "            \n",
    "            # Correct error propagation\n",
    "            error = np.dot(self.layer_weights[-l+1].T, error) * activation_derivative\n",
    "            \n",
    "            bias_gradients[-l] = error\n",
    "            weight_gradients[-l] = np.dot(error, activations[-l-1].T)\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "\n",
    "    \n",
    "    def update_batch(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon=1e-8):\n",
    "        gradient_w = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        gradient_b = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        \n",
    "        for input_val, true_val in batch:\n",
    "            delta_gradient_w, delta_gradient_b = self.backward_propagation(input_val, true_val)\n",
    "            gradient_w = [w + dw for w, dw in zip(gradient_w, delta_gradient_w)]\n",
    "            gradient_b = [b + db for b, db in zip(gradient_b, delta_gradient_b)]\n",
    "\n",
    "        # Update rule for weights and biases based on the optimization method\n",
    "        if optimization_method == 'momentum':\n",
    "            # Momentum initialization\n",
    "            if not hasattr(self, 'velocity_weights'):\n",
    "                self.velocity_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.velocity_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "\n",
    "            # Update velocities\n",
    "            self.velocity_weights = [beta * vw + (1 - beta) * gw / len(batch) for vw, gw in zip(self.velocity_weights, gradient_w)]\n",
    "            self.velocity_biases = [beta * vb + (1 - beta) * gb / len(batch) for vb, gb in zip(self.velocity_biases, gradient_b)]\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - learn_rate * vw\n",
    "                                  for w, vw in zip(self.layer_weights, self.velocity_weights)]\n",
    "            self.layer_biases = [b - learn_rate * vb for b, vb in zip(self.layer_biases, self.velocity_biases)]\n",
    "        elif optimization_method == 'rmsprop':\n",
    "            # RMSprop initialization\n",
    "            if not hasattr(self, 'squared_gradients_weights'):\n",
    "                self.squared_gradients_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.squared_gradients_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "\n",
    "            # Update squared gradients\n",
    "            self.squared_gradients_weights = [beta * sgw + (1 - beta) * (gw**2) / len(batch)\n",
    "                                              for sgw, gw in zip(self.squared_gradients_weights, gradient_w)]\n",
    "            self.squared_gradients_biases = [beta * sgb + (1 - beta) * (gb**2) / len(batch)\n",
    "                                             for sgb, gb in zip(self.squared_gradients_biases, gradient_b)]\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - \n",
    "                                  learn_rate * gw / (np.sqrt(sgw) + epsilon)\n",
    "                                  for w, sgw, gw in zip(self.layer_weights, self.squared_gradients_weights, gradient_w)]\n",
    "            self.layer_biases = [b - learn_rate * gb / (np.sqrt(sgb) + epsilon)\n",
    "                                 for b, sgb, gb in zip(self.layer_biases, self.squared_gradients_biases, gradient_b)]\n",
    "\n",
    "    def train(self, training_data, epochs, learn_rate, batch_size, regularization=0.0, optimization_method='rmsprop', beta=0.9, epsilon=1e-8, visual_interval=10, X_val=None, y_val=None, target = None,adaptive_learn_rate = True, decay_rate=0.1, decay_step=100):\n",
    "        n = len(training_data)\n",
    "        \n",
    "        # Determine mini-batch size based on whether the batch_size_input is a percentage or fixed value\n",
    "        if isinstance(batch_size, float):  # If batch_size_input is a float, treat it as a percentage\n",
    "            batch_size = max(1, min(n, int(n * batch_size / 100)))\n",
    "        elif isinstance(batch_size, int):  # If batch_size_input is an integer, treat it as a fixed size\n",
    "            batch_size = max(1, min(n, batch_size))\n",
    "        else:  # Raise an error if batch_size_input is neither float nor int\n",
    "            raise ValueError(\"batch_size_input must be an integer (fixed size) or a float (percentage of dataset)\")\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k + batch_size] for k in range(0, n, batch_size)]\n",
    "    \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_batch(mini_batch, learn_rate, regularization, n, optimization_method, beta, epsilon)\n",
    "            if adaptive_learn_rate:\n",
    "                # Decay the learning rate every decay_step epochs\n",
    "                if epoch % decay_step == 0 and epoch > 0:\n",
    "                    learn_rate *= (1. / (1. + decay_rate * epoch))\n",
    "    \n",
    "            if epoch % visual_interval == 0:\n",
    "                predictions = np.argmax(np.array([self.propagate_forward(x.reshape(-1, 1))[0] for x in X_val]), axis=1)\n",
    "                accuracy = np.mean(predictions == y_val)\n",
    "                print(f'epoch: {epoch}', f'Test accuracy: {accuracy}')\n",
    "                f1_weighted = f1_score(y_val, predictions, average='weighted')\n",
    "                print(f\"F1 Score (Weighted): {f1_weighted}\")\n",
    "                \n",
    "                if f1_weighted > target:\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.890118Z",
     "start_time": "2024-04-02T11:47:20.886632Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataScaler:\n",
    "    def __init__(self, method=\"standardization\"):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.fit_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.fit_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.inverse_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.inverse_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def fit_transform_min_max(self, data):\n",
    "        self.min = np.min(data, axis=0)\n",
    "        self.max = np.max(data, axis=0)\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def transform_min_max(self, data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse_transform_min_max(self, data):\n",
    "        return data * (self.max - self.min) + self.min\n",
    "\n",
    "    def fit_transform_standardization(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def transform_standardization(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform_standardization(self, data):\n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:20.891984Z",
     "start_time": "2024-04-02T11:47:20.890561Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse(mse_history):\n",
    "    plt.plot(mse_history)\n",
    "    plt.title('MSE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:21.284727Z",
     "start_time": "2024-04-02T11:47:21.276752Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_rings3_regular = pd.read_csv('./data/classification/rings3-regular-training.csv')\n",
    "df_test_rings3_regular = pd.read_csv('./data/classification/rings3-regular-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:21.449977Z",
     "start_time": "2024-04-02T11:47:21.445971Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_easy = pd.read_csv('./data/classification/easy-training.csv')\n",
    "df_test_easy = pd.read_csv('./data/classification/easy-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:21.639963Z",
     "start_time": "2024-04-02T11:47:21.635766Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_xor3 = pd.read_csv('./data/classification/xor3-training.csv')\n",
    "df_test_xor3 = pd.read_csv('./data/classification/xor3-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rings 3 regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:21.959414Z",
     "start_time": "2024-04-02T11:47:21.957508Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler_X = DataScaler(\"standardization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:22.141939Z",
     "start_time": "2024-04-02T11:47:22.139283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_rings = df_train_rings3_regular[['x']].values.reshape(-1, 1)\n",
    "X1_test_rings = df_test_rings3_regular[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:22.338230Z",
     "start_time": "2024-04-02T11:47:22.335917Z"
    }
   },
   "outputs": [],
   "source": [
    "X2_train_rings = df_train_rings3_regular[['y']].values.reshape(-1, 1)\n",
    "X2_test_rings = df_test_rings3_regular[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:22.488955Z",
     "start_time": "2024-04-02T11:47:22.487110Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rings = np.hstack((X1_train_rings, X2_train_rings))\n",
    "X_test_rings = np.hstack((X1_test_rings, X2_test_rings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:22.677555Z",
     "start_time": "2024-04-02T11:47:22.674860Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rings_scaled = np.hstack((scaler_X.fit_transform(X1_train_rings), scaler_X.fit_transform(X2_train_rings)))\n",
    "X_test_rings_scaled = np.hstack((scaler_X.transform(X1_test_rings), scaler_X.transform(X2_test_rings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:22.848817Z",
     "start_time": "2024-04-02T11:47:22.846896Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_rings = df_train_rings3_regular['c'].values.reshape(-1, 1)\n",
    "y_test_rings = df_test_rings3_regular['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:23.034269Z",
     "start_time": "2024-04-02T11:47:23.031001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_rings = encoder.fit_transform(y_train_rings)\n",
    "y_test_encoded_rings = encoder.transform(y_test_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:23.382945Z",
     "start_time": "2024-04-02T11:47:23.379934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_rings = y_train_encoded_rings.shape[1] \n",
    "num_classes_rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:24.035054Z",
     "start_time": "2024-04-02T11:47:24.032547Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_rings = [\n",
    "    (X_train_rings[i].reshape(-1, 1), y_train_encoded_rings[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_rings))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:26.644335Z",
     "start_time": "2024-04-02T11:47:24.520920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.594\n",
      "F1 Score (Weighted): 0.5248995535714286\n",
      "epoch: 10 Test accuracy: 0.488\n",
      "F1 Score (Weighted): 0.4598331159446349\n",
      "epoch: 20 Test accuracy: 0.6685\n",
      "F1 Score (Weighted): 0.6736366350310959\n",
      "epoch: 30 Test accuracy: 0.6715\n",
      "F1 Score (Weighted): 0.6510555245498142\n",
      "epoch: 40 Test accuracy: 0.777\n",
      "F1 Score (Weighted): 0.7769461690399452\n"
     ]
    }
   ],
   "source": [
    "mlp_rings = MLP(sizes=[2, 10, 10, 3], activation_fn=sigmoid,\n",
    "                activation_fn_derivative=sigmoid_derivative)  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "\n",
    "mlp_rings.train(training_data=training_data_rings, epochs=1000, learn_rate=0.01, batch_size=64, X_val=X_test_rings,\n",
    "                y_val=y_test_rings, visual_interval=10, target=0.75, decay_rate=0.01, adaptive_learn_rate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:36.545839Z",
     "start_time": "2024-04-02T11:47:36.481496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_rings = np.argmax(np.array([mlp_rings.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_rings]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:47:36.895929Z",
     "start_time": "2024-04-02T11:47:36.893880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy or other metrics\n",
    "accuracy_rings = np.mean(predictions_rings == y_test_rings)\n",
    "print(f'Test accuracy: {accuracy_rings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:01.625333Z",
     "start_time": "2024-04-02T11:48:01.620210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.7769461690399452\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted_rings = f1_score(y_test_rings, predictions_rings, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_rings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### easy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:01.965327Z",
     "start_time": "2024-04-02T11:48:01.962369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_easy = df_train_easy[['x']].values.reshape(-1, 1)\n",
    "X1_test_easy = df_test_easy[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:02.304235Z",
     "start_time": "2024-04-02T11:48:02.301935Z"
    }
   },
   "outputs": [],
   "source": [
    "X2_train_easy = df_train_easy[['y']].values.reshape(-1, 1)\n",
    "X2_test_easy = df_test_easy[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:02.468533Z",
     "start_time": "2024-04-02T11:48:02.466369Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_easy= np.hstack((X1_train_easy, X2_train_easy))\n",
    "X_test_easy = np.hstack((X1_test_easy, X2_test_easy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:02.634032Z",
     "start_time": "2024-04-02T11:48:02.632119Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_easy = df_train_easy['c'].values.reshape(-1, 1)\n",
    "y_test_easy = df_test_easy['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:02.786001Z",
     "start_time": "2024-04-02T11:48:02.783140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_easy = encoder.fit_transform(y_train_easy)\n",
    "y_test_encoded_easy = encoder.transform(y_test_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:02.942032Z",
     "start_time": "2024-04-02T11:48:02.939556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_easy = y_train_encoded_easy.shape[1] \n",
    "num_classes_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:03.087760Z",
     "start_time": "2024-04-02T11:48:03.085846Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_easy = [\n",
    "    (X_train_easy[i].reshape(-1, 1), y_train_encoded_easy[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_easy))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:03.589288Z",
     "start_time": "2024-04-02T11:48:03.358377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.92\n",
      "F1 Score (Weighted): 0.9198461538461538\n",
      "epoch: 10 Test accuracy: 0.988\n",
      "F1 Score (Weighted): 0.9879994239907839\n",
      "epoch: 20 Test accuracy: 0.998\n",
      "F1 Score (Weighted): 0.9979999599959996\n"
     ]
    }
   ],
   "source": [
    "mlp_easy = MLP(sizes=[2, 2, 2], activation_fn=sigmoid, activation_fn_derivative=sigmoid_derivative)  \n",
    "\n",
    "# Train the MLP using your training data\n",
    "mlp_easy.train(training_data=training_data_easy, epochs=100, learn_rate=0.01, batch_size=20, X_val=X_test_easy, y_val=y_test_easy, visual_interval=10, target = 0.99, decay_rate=0.001, adaptive_learn_rate=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_easy = np.argmax(np.array([mlp_easy.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_easy]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:03.886891Z",
     "start_time": "2024-04-02T11:48:03.871987Z"
    }
   },
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:07.965050Z",
     "start_time": "2024-04-02T11:48:07.962456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_easy = np.mean(predictions_easy == y_test_easy)\n",
    "print(f'Test accuracy: {accuracy_easy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:08.373940Z",
     "start_time": "2024-04-02T11:48:08.370716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.9979999599959996\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted_easy = f1_score(y_test_easy, predictions_easy, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_easy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_xor3 = df_train_xor3[['x']].values.reshape(-1, 1)\n",
    "X1_test_xor3 = df_test_xor3[['x']].values.reshape(-1, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:09.084493Z",
     "start_time": "2024-04-02T11:48:09.081533Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X2_train_xor3 = df_train_xor3[['y']].values.reshape(-1, 1)\n",
    "X2_test_xor3 = df_test_xor3[['y']].values.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:09.595345Z",
     "start_time": "2024-04-02T11:48:09.593049Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_xor3= np.hstack((X1_train_xor3, X2_train_xor3))\n",
    "X_test_xor3 = np.hstack((X1_test_xor3, X2_test_xor3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:09.796624Z",
     "start_time": "2024-04-02T11:48:09.794919Z"
    }
   },
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train_xor3 = df_train_xor3['c'].values.reshape(-1, 1)\n",
    "y_test_xor3 = df_test_xor3['c'].values.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:09.968424Z",
     "start_time": "2024-04-02T11:48:09.966551Z"
    }
   },
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_xor3 = encoder.fit_transform(y_train_xor3)\n",
    "y_test_encoded_xor3 = encoder.transform(y_test_xor3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:10.148561Z",
     "start_time": "2024-04-02T11:48:10.145615Z"
    }
   },
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_xor3 = y_train_encoded_xor3.shape[1] \n",
    "num_classes_xor3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:10.328415Z",
     "start_time": "2024-04-02T11:48:10.326352Z"
    }
   },
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data_xor3 = [\n",
    "    (X_train_xor3[i].reshape(-1, 1), y_train_encoded_xor3[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_xor3))\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:10.765822Z",
     "start_time": "2024-04-02T11:48:10.763690Z"
    }
   },
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.588\n",
      "F1 Score (Weighted): 0.4354458438287153\n",
      "epoch: 10 Test accuracy: 0.436\n",
      "F1 Score (Weighted): 0.44016125765539854\n",
      "epoch: 20 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4904390243902439\n",
      "epoch: 30 Test accuracy: 0.64\n",
      "F1 Score (Weighted): 0.6379420665801989\n",
      "epoch: 40 Test accuracy: 0.64\n",
      "F1 Score (Weighted): 0.6425146527498996\n",
      "epoch: 50 Test accuracy: 0.718\n",
      "F1 Score (Weighted): 0.7188225419369131\n",
      "epoch: 60 Test accuracy: 0.766\n",
      "F1 Score (Weighted): 0.76375059529059\n",
      "epoch: 70 Test accuracy: 0.754\n",
      "F1 Score (Weighted): 0.753722164993236\n",
      "epoch: 80 Test accuracy: 0.764\n",
      "F1 Score (Weighted): 0.758472072072072\n",
      "epoch: 90 Test accuracy: 0.762\n",
      "F1 Score (Weighted): 0.7615414562193651\n",
      "epoch: 100 Test accuracy: 0.764\n",
      "F1 Score (Weighted): 0.7628199999999999\n",
      "epoch: 110 Test accuracy: 0.746\n",
      "F1 Score (Weighted): 0.7468814566253262\n",
      "epoch: 120 Test accuracy: 0.754\n",
      "F1 Score (Weighted): 0.7519035174868268\n",
      "epoch: 130 Test accuracy: 0.75\n",
      "F1 Score (Weighted): 0.7497176473508496\n",
      "epoch: 140 Test accuracy: 0.76\n",
      "F1 Score (Weighted): 0.7569985316703516\n",
      "epoch: 150 Test accuracy: 0.764\n",
      "F1 Score (Weighted): 0.7615993274485078\n",
      "epoch: 160 Test accuracy: 0.75\n",
      "F1 Score (Weighted): 0.747869428340271\n",
      "epoch: 170 Test accuracy: 0.762\n",
      "F1 Score (Weighted): 0.7585827490044899\n",
      "epoch: 180 Test accuracy: 0.762\n",
      "F1 Score (Weighted): 0.7615414562193651\n",
      "epoch: 190 Test accuracy: 0.754\n",
      "F1 Score (Weighted): 0.7528846997697742\n",
      "epoch: 200 Test accuracy: 0.75\n",
      "F1 Score (Weighted): 0.7505822813928746\n",
      "epoch: 210 Test accuracy: 0.772\n",
      "F1 Score (Weighted): 0.7714667860874758\n",
      "epoch: 220 Test accuracy: 0.778\n",
      "F1 Score (Weighted): 0.7763417930745893\n",
      "epoch: 230 Test accuracy: 0.768\n",
      "F1 Score (Weighted): 0.7672600172768955\n",
      "epoch: 240 Test accuracy: 0.754\n",
      "F1 Score (Weighted): 0.751635241202928\n",
      "epoch: 250 Test accuracy: 0.75\n",
      "F1 Score (Weighted): 0.7495183363648794\n",
      "epoch: 260 Test accuracy: 0.75\n",
      "F1 Score (Weighted): 0.7513364513800823\n",
      "epoch: 270 Test accuracy: 0.766\n",
      "F1 Score (Weighted): 0.76375059529059\n",
      "epoch: 280 Test accuracy: 0.76\n",
      "F1 Score (Weighted): 0.7585695921467804\n",
      "epoch: 290 Test accuracy: 0.742\n",
      "F1 Score (Weighted): 0.7431546525742732\n",
      "epoch: 300 Test accuracy: 0.746\n",
      "F1 Score (Weighted): 0.7468814566253262\n",
      "epoch: 310 Test accuracy: 0.754\n",
      "F1 Score (Weighted): 0.7539095584499276\n",
      "epoch: 320 Test accuracy: 0.716\n",
      "F1 Score (Weighted): 0.7177739605699786\n",
      "epoch: 330 Test accuracy: 0.736\n",
      "F1 Score (Weighted): 0.7366935960591133\n",
      "epoch: 340 Test accuracy: 0.748\n",
      "F1 Score (Weighted): 0.7496987673343606\n",
      "epoch: 350 Test accuracy: 0.764\n",
      "F1 Score (Weighted): 0.7653733467994036\n",
      "epoch: 360 Test accuracy: 0.754\n",
      "F1 Score (Weighted): 0.7556893068281699\n",
      "epoch: 370 Test accuracy: 0.758\n",
      "F1 Score (Weighted): 0.759060914823341\n",
      "epoch: 380 Test accuracy: 0.74\n",
      "F1 Score (Weighted): 0.7394133555231954\n",
      "epoch: 390 Test accuracy: 0.768\n",
      "F1 Score (Weighted): 0.7692641025641026\n",
      "epoch: 400 Test accuracy: 0.756\n",
      "F1 Score (Weighted): 0.7566247999487868\n",
      "epoch: 410 Test accuracy: 0.776\n",
      "F1 Score (Weighted): 0.77768605249614\n",
      "epoch: 420 Test accuracy: 0.778\n",
      "F1 Score (Weighted): 0.7794021052631579\n",
      "epoch: 430 Test accuracy: 0.772\n",
      "F1 Score (Weighted): 0.773474130047649\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m mlp_xor3 \u001B[38;5;241m=\u001B[39m MLP(sizes\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m2\u001B[39m], activation_fn\u001B[38;5;241m=\u001B[39msigmoid, activation_fn_derivative\u001B[38;5;241m=\u001B[39msigmoid_derivative)  \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Train the MLP using your training data\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mmlp_xor3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_data_xor3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m96\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_test_xor3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_test_xor3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisual_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.97\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecay_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0005\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madaptive_learn_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 115\u001B[0m, in \u001B[0;36mMLP.train\u001B[0;34m(self, training_data, epochs, learn_rate, batch_size, regularization, optimization_method, beta, epsilon, visual_interval, X_val, y_val, target, adaptive_learn_rate, decay_rate, decay_step)\u001B[0m\n\u001B[1;32m    112\u001B[0m mini_batches \u001B[38;5;241m=\u001B[39m [training_data[k:k \u001B[38;5;241m+\u001B[39m batch_size] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, n, batch_size)]\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregularization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimization_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m adaptive_learn_rate:\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;66;03m# Decay the learning rate every decay_step epochs\u001B[39;00m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m decay_step \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[12], line 60\u001B[0m, in \u001B[0;36mMLP.update_batch\u001B[0;34m(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon)\u001B[0m\n\u001B[1;32m     57\u001B[0m gradient_b \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mzeros(bias\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m bias \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_biases]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_val, true_val \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[0;32m---> 60\u001B[0m     delta_gradient_w, delta_gradient_b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward_propagation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrue_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     gradient_w \u001B[38;5;241m=\u001B[39m [w \u001B[38;5;241m+\u001B[39m dw \u001B[38;5;28;01mfor\u001B[39;00m w, dw \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(gradient_w, delta_gradient_w)]\n\u001B[1;32m     62\u001B[0m     gradient_b \u001B[38;5;241m=\u001B[39m [b \u001B[38;5;241m+\u001B[39m db \u001B[38;5;28;01mfor\u001B[39;00m b, db \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(gradient_b, delta_gradient_b)]\n",
      "Cell \u001B[0;32mIn[12], line 31\u001B[0m, in \u001B[0;36mMLP.backward_propagation\u001B[0;34m(self, input_val, true_val)\u001B[0m\n\u001B[1;32m     28\u001B[0m bias_gradients \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mzeros(bias\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m bias \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_biases]\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Forward pass to get activations\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m final_act, activations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpropagate_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Start with the derivative of the loss function w.r.t. the final activation\u001B[39;00m\n\u001B[1;32m     34\u001B[0m error \u001B[38;5;241m=\u001B[39m cross_entropy_derivative(final_act, true_val)\n",
      "Cell \u001B[0;32mIn[12], line 21\u001B[0m, in \u001B[0;36mMLP.propagate_forward\u001B[0;34m(self, input_activation)\u001B[0m\n\u001B[1;32m     19\u001B[0m     activations\u001B[38;5;241m.\u001B[39mappend(input_activation)\n\u001B[1;32m     20\u001B[0m final_input \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_weights[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], input_activation) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_biases[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m---> 21\u001B[0m output_activation \u001B[38;5;241m=\u001B[39m \u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfinal_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m activations\u001B[38;5;241m.\u001B[39mappend(output_activation)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# change\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m, in \u001B[0;36msoftmax\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msoftmax\u001B[39m(x):\n\u001B[1;32m      2\u001B[0m     exp_x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(x \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(x))  \u001B[38;5;66;03m# Subtract max for numerical stability\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m exp_x \u001B[38;5;241m/\u001B[39m \u001B[43mexp_x\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:49\u001B[0m, in \u001B[0;36m_sum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sum\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     48\u001B[0m          initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mumr_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "mlp_xor3 = MLP(sizes=[2, 15, 15, 2], activation_fn=sigmoid, activation_fn_derivative=sigmoid_derivative)  \n",
    "\n",
    "# Train the MLP using your training data\n",
    "mlp_xor3.train(training_data=training_data_xor3, epochs=10000, learn_rate=0.001, batch_size=96, X_val=X_test_xor3, y_val=y_test_xor3, visual_interval=10, target = 0.97, decay_rate=0.0005, adaptive_learn_rate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:48:18.503011Z",
     "start_time": "2024-04-02T11:48:11.073781Z"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_xor3 = np.argmax(np.array([mlp_xor3.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_xor3]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_xor3 = np.mean(predictions_xor3 == y_test_xor3)\n",
    "print(f'Test accuracy: {accuracy_xor3}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.9940000720002881\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted_xor3 = f1_score(y_test_xor3, predictions_xor3, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_easy}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
