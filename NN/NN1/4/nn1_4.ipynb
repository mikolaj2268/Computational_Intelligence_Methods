{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:04:16.951760Z",
     "start_time": "2024-03-25T20:04:16.945151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikolajmroz/Developer/Computational_Intelligence_Methods\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/mikolajmroz/Developer/Computational_Intelligence_Methods')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:56.641831Z",
     "start_time": "2024-03-25T21:01:56.637035Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:56.777619Z",
     "start_time": "2024-03-25T21:01:56.775809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:56.934066Z",
     "start_time": "2024-03-25T21:01:56.932059Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:57.065344Z",
     "start_time": "2024-03-25T21:01:57.063390Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:57.246083Z",
     "start_time": "2024-03-25T21:01:57.243976Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # Corrected sigmoid derivative to properly compute the derivative\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:57.396606Z",
     "start_time": "2024-03-25T21:01:57.394568Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:57.566474Z",
     "start_time": "2024-03-25T21:01:57.564291Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:57.726204Z",
     "start_time": "2024-03-25T21:01:57.724102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(softmax_output, y_true):\n",
    "    # Assuming y_true is one-hot encoded\n",
    "    m = y_true.shape[1]  # Number of examples\n",
    "    log_likelihood = -np.log(softmax_output[y_true.argmax(axis=0), range(m)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:57.890292Z",
     "start_time": "2024-03-25T21:01:57.888263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss_derivative(softmax_output, y_true):\n",
    "\n",
    "    corrected_softmax_output = softmax_output - y_true\n",
    "    \n",
    "    return corrected_softmax_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:58.041373Z",
     "start_time": "2024-03-25T21:01:58.039389Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_derivative(softmax_output):\n",
    "    # For softmax combined with cross-entropy loss, the derivative simplifies\n",
    "    # the gradient calculation in backpropagation, directly using output error.\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:58.201810Z",
     "start_time": "2024-03-25T21:01:58.188669Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron (MLP) Class:\n",
    "    A simple implementation of a feedforward neural network that uses backpropagation for training.\n",
    "\n",
    "    Attributes:\n",
    "    - layer_sizes (list): The sizes of the layers in the neural network.\n",
    "    - layer_weights (list): Weights for each layer in the neural network, initialized based on He initialization.\n",
    "    - layer_biases (list): Biases for each layer in the neural network, initialized to zeros.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes, activation_fn=relu, activation_fn_derivative=relu_derivative):\n",
    "        \"\"\"\n",
    "        Initializes a new MLP instance.\n",
    "        \n",
    "        Parameters:\n",
    "        - sizes (list): A list containing the size (number of neurons) of each layer in the network.\n",
    "        - activation_fn (str): The name of the activation function to use ('sigmoid' by default).\n",
    "        \"\"\"\n",
    "        self.layer_sizes = sizes\n",
    "        \n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_derivative = activation_fn_derivative\n",
    "        \n",
    "        # Initialize weights with He initialization for ReLU activations; suitable for layers not using ReLU, but common.\n",
    "        self.layer_weights = [\n",
    "            np.random.randn(next_layer, prev_layer) * np.sqrt(2.0 / prev_layer)\n",
    "            for prev_layer, next_layer in zip(sizes[:-1], sizes[1:])\n",
    "        ]\n",
    "        \n",
    "        # Initialize biases for all layers (except input layer) to zeros.\n",
    "        self.layer_biases = [np.zeros((neurons, 1)) for neurons in sizes[1:]]\n",
    "\n",
    "    def display_weights_biases(self):\n",
    "        \"\"\"Prints the weights and biases of each layer in the network.\"\"\"\n",
    "        print(\"Final Weights and Biases:\")\n",
    "        for layer_index, (weights, biases) in enumerate(zip(self.layer_weights, self.layer_biases)):\n",
    "            print(f\"Layer {layer_index + 1} Weights:\\n{weights}\")\n",
    "            print(f\"Layer {layer_index + 1} Biases:\\n{biases}\")\n",
    "\n",
    "    def propagate_forward(self, input_activation):\n",
    "        activations = [input_activation]\n",
    "        for biases, weights in zip(self.layer_biases[:-1], self.layer_weights[:-1]):\n",
    "            input_activation = self.activation_fn(np.dot(weights, input_activation) + biases)\n",
    "            activations.append(input_activation)\n",
    "        # Use softmax for the final layer for multi-class classification\n",
    "        final_input = np.dot(self.layer_weights[-1], input_activation) + self.layer_biases[-1]\n",
    "\n",
    "        final_output = softmax(final_input)\n",
    "        activations.append(final_output)\n",
    "        return final_output, activations\n",
    "\n",
    "    def backward_propagation(self, input_val, true_val):\n",
    "        weight_gradients = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        bias_gradients = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        \n",
    "        final_act, activations = self.propagate_forward(input_val)\n",
    "        \n",
    "        # Error at the output layer for softmax combined with cross-entropy\n",
    "        error = cross_entropy_loss_derivative(final_act, true_val)\n",
    "        bias_gradients[-1] = error\n",
    "        weight_gradients[-1] = np.dot(error, activations[-2].T)\n",
    "        \n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            # Calculate 'z' using the transposed weights of the next layer and the current error\n",
    "            z = np.dot(self.layer_weights[-l+1].T, error)\n",
    "            # Recalculate error for the current layer\n",
    "            error = self.activation_fn_derivative(z)\n",
    "            # Update gradients\n",
    "            bias_gradients[-l] = error\n",
    "            weight_gradients[-l] = np.dot(error, activations[-l-1].T)\n",
    "\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def update_batch(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon=None):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of the network for a single batch using gradient descent with regularization\n",
    "        and optionally applies momentum or RMSprop as optimization methods to accelerate convergence.\n",
    "    \n",
    "        Parameters:\n",
    "        - batch (list of tuples): Each tuple contains input data and true labels/values for a batch of samples.\n",
    "        - learn_rate (float): Learning rate for the optimization.\n",
    "        - regularization (float): Regularization factor to reduce overfitting by penalizing large weights.\n",
    "        - total_size (int): Total number of samples in the dataset, used for regularization calculation.\n",
    "        - optimization_method (str): Specifies the optimization method ('momentum' or 'rmsprop').\n",
    "        - beta (float): Hyperparameter for the optimization methods, affecting the weighting of past gradients.\n",
    "        - epsilon (float, optional): A small number to avoid division by zero in 'rmsprop', default is None.\n",
    "    \n",
    "        Returns:\n",
    "        - None, but updates the network's weights and biases in place.\n",
    "        \"\"\"\n",
    "        # Initialize gradients for weights and biases with zeros\n",
    "        gradient_w = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        gradient_b = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        \n",
    "        # Loop through each data point in the batch\n",
    "        for input_val, true_val in batch:\n",
    "            # Perform backpropagation to compute gradients for current data point\n",
    "            delta_gradient_w, delta_gradient_b = self.backward_propagation(input_val, true_val)\n",
    "            # Accumulate gradients over the batch\n",
    "            gradient_w = [w + dw for w, dw in zip(gradient_w, delta_gradient_w)]\n",
    "            gradient_b = [b + db for b, db in zip(gradient_b, delta_gradient_b)]\n",
    "\n",
    "        # Apply optimization method if specified\n",
    "        if optimization_method == 'momentum':\n",
    "            # Initialize velocity terms for weights and biases if not already initialized\n",
    "            if not hasattr(self, 'velocity_weights'):\n",
    "                self.velocity_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.velocity_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "                \n",
    "            # Update velocities based on gradients and apply to weights and biases\n",
    "            self.velocity_weights = [beta * vw + (1 - beta) * gw for vw, gw in zip(self.velocity_weights, gradient_w)]\n",
    "            self.velocity_biases = [beta * vb + (1 - beta) * gb for vb, gb in zip(self.velocity_biases, gradient_b)]\n",
    "            # Update weights and biases using momentum method\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - learn_rate * vw\n",
    "                                  for w, vw in zip(self.layer_weights, self.velocity_weights)]\n",
    "            self.layer_biases = [b - learn_rate * vb for b, vb in zip(self.layer_biases, self.velocity_biases)]\n",
    "            \n",
    "        elif optimization_method == 'rmsprop':\n",
    "            # Initialize squared gradient terms for weights and biases if not already initialized\n",
    "            if not hasattr(self, 'squared_gradients_weights'):\n",
    "                self.squared_gradients_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.squared_gradients_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "                \n",
    "            # Update squared gradients based on new gradients\n",
    "            self.squared_gradients_weights = [beta * sgw + (1 - beta) * (gw**2)\n",
    "                                              for sgw, gw in zip(self.squared_gradients_weights, gradient_w)]\n",
    "            self.squared_gradients_biases = [beta * sgb + (1 - beta) * (gb**2)\n",
    "                                             for sgb, gb in zip(self.squared_gradients_biases, gradient_b)]\n",
    "            # Update weights and biases using rmsprop method\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - \n",
    "                                  (learn_rate / (np.sqrt(sgw) + epsilon)) * gw\n",
    "                                  for w, sgw, gw in zip(self.layer_weights, self.squared_gradients_weights, gradient_w)]\n",
    "            self.layer_biases = [b - (learn_rate / (np.sqrt(sgb) + epsilon)) * gb\n",
    "                                 for b, sgb, gb in zip(self.layer_biases, self.squared_gradients_biases, gradient_b)]\n",
    "    \n",
    "    def train(self, training_set, epoch_count, learn_rate, batch_size_input, regularization=0.0,\n",
    "              visual_interval=100, optimization_method='momentum', beta=0.9, epsilon=1e-8, \n",
    "              X_val_scaled=None, y_val_scaled=None, X_val=None, y_val=None,\n",
    "              scaler_X=None, scaler_y=None, mse_limit=None):\n",
    "        \"\"\"\n",
    "        Trains the neural network over a specified number of epochs using mini-batch gradient descent,\n",
    "        and tracks performance over epochs through mean squared error on a validation set. Supports learning\n",
    "        rate scheduling, regularization, and early stopping.\n",
    "    \n",
    "        Parameters:\n",
    "        - training_set (list): Training data consisting of tuples of input values and true labels.\n",
    "        - epoch_count (int): Total number of epochs to train the network.\n",
    "        - learn_rate (float): Initial learning rate for optimization.\n",
    "        - batch_size_input (int or float): Size of the mini-batches for training. If float, it represents\n",
    "          the percentage of the total dataset size.\n",
    "        - regularization (float): Regularization factor for reducing overfitting.\n",
    "        - visual_interval (int): Frequency of epochs at which to calculate and print the MSE for tracking.\n",
    "        - optimization_method (str): Optimization method to use ('momentum' or 'rmsprop').\n",
    "        - beta (float): Parameter for the optimization method that controls the momentum or the weighted average.\n",
    "        - epsilon (float): Small value to prevent division by zero in 'rmsprop'.\n",
    "        - X_val_scaled, y_val_scaled, X_val, y_val (ndarray, optional): Validation datasets for performance evaluation.\n",
    "        - scaler_X, scaler_y (preprocessing scaler objects, optional): Scalers used for transforming data back to original scale for MSE calculation.\n",
    "        - mse_limit (float, optional): MSE value at which training can be stopped early for performance.\n",
    "    \n",
    "        Returns:\n",
    "        - mse_history (list): History of mean squared error values at each visual interval.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Initialize an empty list to store the history of mean squared errors (MSE) for each epoch\n",
    "        loss_history = []\n",
    "        # Determine the total number of samples in the training set\n",
    "        data_size = len(training_set)\n",
    "        # Store the initial learning rate, which may be adjusted during training\n",
    "        rate_init = learn_rate\n",
    "    \n",
    "        # Determine mini-batch size based on whether the batch_size_input is a percentage or fixed value\n",
    "        if isinstance(batch_size_input, float):  # If batch_size_input is a float, treat it as a percentage\n",
    "            mini_batch_size = max(1, min(data_size, int(data_size * batch_size_input / 100)))\n",
    "        elif isinstance(batch_size_input, int):  # If batch_size_input is an integer, treat it as a fixed size\n",
    "            mini_batch_size = max(1, min(data_size, batch_size_input))\n",
    "        else:  # Raise an error if batch_size_input is neither float nor int\n",
    "            raise ValueError(\"batch_size_input must be an integer (fixed size) or a float (percentage of dataset)\")\n",
    "    \n",
    "        # Iterate through each epoch for training\n",
    "        for epoch in range(epoch_count):\n",
    "            # Randomly shuffle the training set to ensure randomness of mini-batches\n",
    "            np.random.shuffle(training_set)\n",
    "            # Create mini-batches from the training set\n",
    "            mini_batches = [training_set[k:k + mini_batch_size] for k in range(0, data_size, mini_batch_size)]\n",
    "            # Update the model's weights and biases for each mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_batch(mini_batch, learn_rate, regularization, data_size, optimization_method, beta, epsilon)\n",
    "    \n",
    "            # Adjust the learning rate after each epoch (simple learning rate decay)\n",
    "            learn_rate = rate_init / (1 + 0.01 * epoch)\n",
    "    \n",
    "            # At specified intervals or the last epoch, evaluate and print the model's performance\n",
    "            if epoch % visual_interval == 0 or epoch == epoch_count - 1:\n",
    "                '''\n",
    "                # Generate predictions for the validation set\n",
    "                predictions = np.array([self.propagate_forward(x.reshape(-1, 1))[0] for x in X_val])\n",
    "\n",
    "                # Calculate the cross-entropy loss on the validation set\n",
    "                # Assuming y_val is one-hot encoded and predictions are the output of the softmax layer\n",
    "                cross_entropy_loss_val = cross_entropy_loss(predictions, y_val)\n",
    "\n",
    "                # Replace mse_history with a more appropriately named variable, like loss_history\n",
    "                loss_history.append((epoch, cross_entropy_loss_val))\n",
    "\n",
    "                # Print the current epoch and its cross-entropy loss\n",
    "                print(f'Epoch {epoch}, Cross-Entropy Loss: {cross_entropy_loss_val}')\n",
    "\n",
    "\n",
    "                # If a threshold for MSE is set and the current MSE is below this threshold, stop training\n",
    "                if mse_limit is not None and cross_entropy_loss_val < mse_limit:\n",
    "                    break \n",
    "                '''\n",
    "                predictions = np.argmax(np.array([self.propagate_forward(x.reshape(-1, 1))[0] for x in X_val]), axis=1)\n",
    "                # Calculate accuracy or other metrics\n",
    "                accuracy = np.mean(predictions == y_val)\n",
    "                print(f'epoch: {epoch}', f'Test accuracy: {accuracy}')\n",
    "                \n",
    "                # Calculate F1 Score\n",
    "                f1_weighted = f1_score(y_val, predictions, average='weighted')\n",
    "                print(f\"F1 Score (Weighted): {f1_weighted}\")\n",
    "        # \n",
    "        # # Return the history of MSE values\n",
    "        # return loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataScaler:\n",
    "    def __init__(self, method=\"standardization\"):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.fit_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.fit_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.inverse_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.inverse_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def fit_transform_min_max(self, data):\n",
    "        self.min = np.min(data, axis=0)\n",
    "        self.max = np.max(data, axis=0)\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def transform_min_max(self, data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse_transform_min_max(self, data):\n",
    "        return data * (self.max - self.min) + self.min\n",
    "\n",
    "    def fit_transform_standardization(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def transform_standardization(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform_standardization(self, data):\n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:58.513565Z",
     "start_time": "2024-03-25T21:01:58.511423Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse(mse_history):\n",
    "    plt.plot(mse_history)\n",
    "    plt.title('MSE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:13:02.401107Z",
     "start_time": "2024-03-25T23:13:02.324153Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_train_rings3_regular \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/classification/rings3-regular-training.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m df_test_rings3_regular \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/classification/rings3-regular-test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_rings3_regular = pd.read_csv('./data/classification/rings3-regular-training.csv')\n",
    "df_test_rings3_regular = pd.read_csv('./data/classification/rings3-regular-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_easy = pd.read_csv('./data/classification/easy-training.csv')\n",
    "df_test_easy = pd.read_csv('./data/classification/easy-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_xor3 = pd.read_csv('./data/classification/xor3-training.csv')\n",
    "df_test_xor3 = pd.read_csv('./data/classification/xor3-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rings 3 regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:41.313890Z",
     "start_time": "2024-03-25T22:38:41.312238Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_X = DataScaler(\"standardization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:41.948056Z",
     "start_time": "2024-03-25T22:38:41.942488Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_rings = df_train_rings3_regular[['x']].values.reshape(-1, 1)\n",
    "X1_test_rings = df_test_rings3_regular[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:42.500322Z",
     "start_time": "2024-03-25T22:38:42.497869Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2_train_rings = df_train_rings3_regular[['y']].values.reshape(-1, 1)\n",
    "X2_test_rings = df_test_rings3_regular[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:43.351917Z",
     "start_time": "2024-03-25T22:38:43.349654Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_rings = np.hstack((X1_train_rings, X2_train_rings))\n",
    "X_test_rings = np.hstack((X1_test_rings, X2_test_rings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:43.776986Z",
     "start_time": "2024-03-25T22:38:43.774705Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_rings_scaled = np.hstack((scaler_X.fit_transform(X1_train_rings), scaler_X.fit_transform(X2_train_rings)))\n",
    "X_test_rings_scaled = np.hstack((scaler_X.transform(X1_test_rings), scaler_X.transform(X2_test_rings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:44.125831Z",
     "start_time": "2024-03-25T22:38:44.123884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_rings = df_train_rings3_regular['c'].values.reshape(-1, 1)\n",
    "y_test_rings = df_test_rings3_regular['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:44.512293Z",
     "start_time": "2024-03-25T22:38:44.509350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_rings = encoder.fit_transform(y_train_rings)\n",
    "y_test_encoded_rings = encoder.transform(y_test_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:44.866672Z",
     "start_time": "2024-03-25T22:38:44.864337Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_rings = y_train_encoded_rings.shape[1] \n",
    "num_classes_rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:38:45.298324Z",
     "start_time": "2024-03-25T22:38:45.295549Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_rings = [\n",
    "    (X_train_rings[i].reshape(-1, 1), y_train_encoded_rings[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_rings))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:48:49.622594Z",
     "start_time": "2024-03-25T22:48:25.150268Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[130], line 11\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Train the MLP using your training data\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# This step will vary depending on the exact implementation of your `train` method\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# For example:\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[43mmlp_rings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_data_rings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_train_rings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_train_rings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisual_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[115], line 192\u001B[0m, in \u001B[0;36mMLP.train\u001B[0;34m(self, training_set, epoch_count, learn_rate, batch_size_input, regularization, visual_interval, optimization_method, beta, epsilon, X_val_scaled, y_val_scaled, X_val, y_val, scaler_X, scaler_y, mse_limit)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;66;03m# Update the model's weights and biases for each mini-batch\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregularization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimization_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# Adjust the learning rate after each epoch (simple learning rate decay)\u001B[39;00m\n\u001B[1;32m    195\u001B[0m learn_rate \u001B[38;5;241m=\u001B[39m rate_init \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.01\u001B[39m \u001B[38;5;241m*\u001B[39m epoch)\n",
      "Cell \u001B[0;32mIn[115], line 103\u001B[0m, in \u001B[0;36mMLP.update_batch\u001B[0;34m(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# Loop through each data point in the batch\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_val, true_val \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;66;03m# Perform backpropagation to compute gradients for current data point\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m     delta_gradient_w, delta_gradient_b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward_propagation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrue_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;66;03m# Accumulate gradients over the batch\u001B[39;00m\n\u001B[1;32m    105\u001B[0m     gradient_w \u001B[38;5;241m=\u001B[39m [w \u001B[38;5;241m+\u001B[39m dw \u001B[38;5;28;01mfor\u001B[39;00m w, dw \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(gradient_w, delta_gradient_w)]\n",
      "Cell \u001B[0;32mIn[115], line 69\u001B[0m, in \u001B[0;36mMLP.backward_propagation\u001B[0;34m(self, input_val, true_val)\u001B[0m\n\u001B[1;32m     67\u001B[0m z \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_weights[\u001B[38;5;241m-\u001B[39ml\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mT, error)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Recalculate error for the current layer by multiplying it with the derivative of the activation function\u001B[39;00m\n\u001B[0;32m---> 69\u001B[0m error \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactivation_fn_derivative\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Corrected line\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Update gradients for biases by summing errors across the batch\u001B[39;00m\n\u001B[1;32m     71\u001B[0m bias_gradients[\u001B[38;5;241m-\u001B[39ml] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(error, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Corrected line\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (10,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_rings = MLP(sizes=[2, 10, 3])  # Example layer setup\n",
    "\n",
    "print()\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_rings.train(training_set=training_data_rings, epoch_count=1000, learn_rate=1, batch_size_input=32, X_val=X_train_rings, y_val=y_train_rings, visual_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:48:18.836535Z",
     "start_time": "2024-03-25T22:48:18.813246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_rings = np.argmax(np.array([mlp_rings.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_rings]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:48:19.129228Z",
     "start_time": "2024-03-25T22:48:19.125842Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3975\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy or other metrics\n",
    "accuracy_rings = np.mean(predictions_rings == y_test_rings)\n",
    "print(f'Test accuracy: {accuracy_rings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:39:13.646652Z",
     "start_time": "2024-03-25T22:39:13.642472Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.1731459555511997\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted = f1_score(y_test_rings, predictions_rings, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:51:53.559037Z",
     "start_time": "2024-03-25T21:51:53.555725Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_rings_scaled = [\n",
    "    (X_train_rings_scaled[i].reshape(-1, 1), y_train_encoded_rings[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_rings_scaled))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.617924Z",
     "start_time": "2024-03-25T21:54:26.968078Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_data_rings_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 11\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Train the MLP using your training data\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# This step will vary depending on the exact implementation of your `train` method\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# For example:\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m mlp_rings\u001B[38;5;241m.\u001B[39mtrain(training_set\u001B[38;5;241m=\u001B[39m\u001B[43mtraining_data_rings_scaled\u001B[49m, epoch_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, learn_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, batch_size_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, X_val\u001B[38;5;241m=\u001B[39mX_train_rings, y_val\u001B[38;5;241m=\u001B[39my_train_rings, visual_interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'training_data_rings_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_rings = MLP(sizes=[2, 10, 3])  # Example layer setup\n",
    "\n",
    "print()\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_rings.train(training_set=training_data_rings_scaled, epoch_count=1000, learn_rate=1, batch_size_input=20, X_val=X_train_rings, y_val=y_train_rings, visual_interval=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.637752Z",
     "start_time": "2024-03-25T21:54:49.618892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_rings = np.argmax(np.array([mlp_rings.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_rings]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.642295Z",
     "start_time": "2024-03-25T21:54:49.638805Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.421\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy or other metrics\n",
    "accuracy_rings = np.mean(predictions_rings == y_test_rings)\n",
    "print(f'Test accuracy: {accuracy_rings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.647057Z",
     "start_time": "2024-03-25T21:54:49.644106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.35879514074384916\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted = f1_score(y_test_rings, predictions_rings, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### easy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:30.582502Z",
     "start_time": "2024-03-25T22:07:30.574146Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_easy = df_train_easy[['x']].values.reshape(-1, 1)\n",
    "X1_test_easy = df_test_easy[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:30.743384Z",
     "start_time": "2024-03-25T22:07:30.740755Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2_train_easy = df_train_easy[['y']].values.reshape(-1, 1)\n",
    "X2_test_easy = df_test_easy[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:30.895991Z",
     "start_time": "2024-03-25T22:07:30.893859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_easy= np.hstack((X1_train_easy, X2_train_easy))\n",
    "X_test_easy = np.hstack((X1_test_easy, X2_test_easy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:31.057984Z",
     "start_time": "2024-03-25T22:07:31.056057Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_easy = df_train_easy['c'].values.reshape(-1, 1)\n",
    "y_test_easy = df_test_easy['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:31.210359Z",
     "start_time": "2024-03-25T22:07:31.206021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_easy = encoder.fit_transform(y_train_easy)\n",
    "y_test_encoded_easy = encoder.transform(y_test_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:31.387271Z",
     "start_time": "2024-03-25T22:07:31.384497Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_easy = y_train_encoded_easy.shape[1] \n",
    "num_classes_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:07:31.525828Z",
     "start_time": "2024-03-25T22:07:31.523272Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_easy = [\n",
    "    (X_train_easy[i].reshape(-1, 1), y_train_encoded_easy[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_easy))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.375804Z",
     "start_time": "2024-03-25T22:07:31.933853Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4919674859190989\n",
      "epoch: 100 Test accuracy: 0.488\n",
      "F1 Score (Weighted): 0.4752291772577445\n",
      "epoch: 200 Test accuracy: 0.49\n",
      "F1 Score (Weighted): 0.4769423898194528\n",
      "epoch: 300 Test accuracy: 0.49\n",
      "F1 Score (Weighted): 0.4769423898194528\n",
      "epoch: 400 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 500 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 600 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 700 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 800 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 900 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1000 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1100 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1200 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1300 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1400 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1500 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1600 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1700 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1800 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 1900 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2000 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2100 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2200 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2300 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2400 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2500 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2600 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2700 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2800 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 2900 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3000 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3100 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3200 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3300 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3400 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3500 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3600 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3700 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3800 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 3900 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4000 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4100 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4200 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4300 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4400 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4500 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4600 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4700 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4800 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4900 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n",
      "epoch: 4999 Test accuracy: 0.492\n",
      "F1 Score (Weighted): 0.4786535303776683\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_easy = MLP(sizes=[2, 5, 2])  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_easy.train(training_set=training_data_easy, epoch_count=5000, learn_rate=0.01, batch_size_input=24, X_val=X_test_easy, y_val=y_train_easy, visual_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.390545Z",
     "start_time": "2024-03-25T22:09:21.377572Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_easy = np.argmax(np.array([mlp_easy.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_easy]), axis=1)\n",
    "predictions_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.393675Z",
     "start_time": "2024-03-25T22:09:21.391329Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_easy = np.mean(predictions_easy == y_test_easy)\n",
    "print(f'Test accuracy: {accuracy_easy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.398427Z",
     "start_time": "2024-03-25T22:09:21.394888Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.8032047129104457\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted = f1_score(y_test_easy, predictions_easy, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:15.898651Z",
     "start_time": "2024-03-25T10:00:15.895604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_X = DataScaler(\"standardization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.028622Z",
     "start_time": "2024-03-25T10:00:16.026353Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_train_xor = df_train_xor3[['x', 'y']].values\n",
    "X_test_xor = df_test_xor3[['x', 'y']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.177906Z",
     "start_time": "2024-03-25T10:00:16.175836Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use your custom scaler to scale the X values\n",
    "X_train_scaled_xor = scaler_X.fit_transform(X_train_xor)\n",
    "X_test_scaled_xor = scaler_X.transform(X_test_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.319675Z",
     "start_time": "2024-03-25T10:00:16.315479Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_xor = encoder.fit_transform(df_train_xor3[['c']])\n",
    "y_test_encoded_xor = encoder.transform(df_test_xor3[['c']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.456767Z",
     "start_time": "2024-03-25T10:00:16.454549Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "num_classes_xor = y_train_encoded_xor.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_xor = MLP(sizes=[2, 10, num_classes_xor])  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "# mlp.train(training_set=list(zip(X_train_scaled, y_train_encoded)), epoch_count=100, learn_rate=0.01, batch_size_input=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.601251Z",
     "start_time": "2024-03-25T10:00:16.590068Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_xor = np.argmax(np.array([mlp_xor.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_scaled_xor]), axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test_encoded back to labels for comparison\n",
    "y_test_labels_xor = np.argmax(y_test_encoded_xor, axis=1)\n",
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_xor = np.mean(predictions_xor == y_test_labels_xor)\n",
    "print(f'Test accuracy: {accuracy_xor}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.840031Z",
     "start_time": "2024-03-25T10:00:16.834164Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Macro): 0.3702770780856423\n",
      "F1 Score (Micro): 0.588\n",
      "F1 Score (Weighted): 0.4354458438287153\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_test_labels_xor, predictions_xor, average='macro')\n",
    "f1_micro = f1_score(y_test_labels_xor, predictions_xor, average='micro')\n",
    "f1_weighted = f1_score(y_test_labels_xor, predictions_xor, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
