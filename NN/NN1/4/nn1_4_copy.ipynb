{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:07.400907Z",
     "start_time": "2024-03-25T23:10:07.398314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikolajmroz/Developer/Computational_Intelligence_Methods\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/mikolajmroz/Developer/Computational_Intelligence_Methods')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:07.537667Z",
     "start_time": "2024-03-25T23:10:07.535969Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:07.719447Z",
     "start_time": "2024-03-25T23:10:07.717543Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:07.906662Z",
     "start_time": "2024-03-25T23:10:07.904926Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:08.076126Z",
     "start_time": "2024-03-25T23:10:08.074180Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:08.252526Z",
     "start_time": "2024-03-25T23:10:08.250608Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # Corrected sigmoid derivative to properly compute the derivative\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:08.418934Z",
     "start_time": "2024-03-25T23:10:08.417142Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:08.616009Z",
     "start_time": "2024-03-25T23:10:08.613994Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:08.776556Z",
     "start_time": "2024-03-25T23:10:08.774630Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(softmax_output, y_true):\n",
    "    # Assuming y_true is one-hot encoded\n",
    "    m = y_true.shape[1]  # Number of examples\n",
    "    log_likelihood = -np.log(softmax_output[y_true.argmax(axis=0), range(m)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:08.944701Z",
     "start_time": "2024-03-25T23:10:08.942715Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss_derivative(softmax_output, y_true):\n",
    "\n",
    "    corrected_softmax_output = softmax_output - y_true\n",
    "    \n",
    "    return corrected_softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:10:09.126800Z",
     "start_time": "2024-03-25T23:10:09.124659Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_derivative(softmax_output):\n",
    "    # For softmax combined with cross-entropy loss, the derivative simplifies\n",
    "    # the gradient calculation in backpropagation, directly using output error.\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:18.440217Z",
     "start_time": "2024-03-25T23:14:18.424654Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron (MLP) Class:\n",
    "    A simple implementation of a feedforward neural network that uses backpropagation for training.\n",
    "\n",
    "    Attributes:\n",
    "    - layer_sizes (list): The sizes of the layers in the neural network.\n",
    "    - layer_weights (list): Weights for each layer in the neural network, initialized based on He initialization.\n",
    "    - layer_biases (list): Biases for each layer in the neural network, initialized to zeros.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes, activation_fn=relu, activation_fn_derivative=relu_derivative):\n",
    "        \"\"\"\n",
    "        Initializes a new MLP instance.\n",
    "        \n",
    "        Parameters:\n",
    "        - sizes (list): A list containing the size (number of neurons) of each layer in the network.\n",
    "        - activation_fn (str): The name of the activation function to use ('sigmoid' by default).\n",
    "        \"\"\"\n",
    "        self.layer_sizes = sizes\n",
    "        \n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_derivative = activation_fn_derivative\n",
    "        \n",
    "        # Initialize weights with He initialization for ReLU activations; suitable for layers not using ReLU, but common.\n",
    "        self.layer_weights = [\n",
    "            np.random.randn(next_layer, prev_layer) * np.sqrt(2.0 / prev_layer)\n",
    "            for prev_layer, next_layer in zip(sizes[:-1], sizes[1:])\n",
    "        ]\n",
    "        \n",
    "        # Initialize biases for all layers (except input layer) to zeros.\n",
    "        self.layer_biases = [np.zeros((neurons, 1)) for neurons in sizes[1:]]\n",
    "\n",
    "    def display_weights_biases(self):\n",
    "        \"\"\"Prints the weights and biases of each layer in the network.\"\"\"\n",
    "        print(\"Final Weights and Biases:\")\n",
    "        for layer_index, (weights, biases) in enumerate(zip(self.layer_weights, self.layer_biases)):\n",
    "            print(f\"Layer {layer_index + 1} Weights:\\n{weights}\")\n",
    "            print(f\"Layer {layer_index + 1} Biases:\\n{biases}\")\n",
    "\n",
    "    def propagate_forward(self, input_activation):\n",
    "        activations = [input_activation]\n",
    "        for biases, weights in zip(self.layer_biases[:-1], self.layer_weights[:-1]):\n",
    "            input_activation = self.activation_fn(np.dot(weights, input_activation) + biases)\n",
    "            activations.append(input_activation)\n",
    "        # Use softmax for the final layer for multi-class classification\n",
    "        final_input = np.dot(self.layer_weights[-1], input_activation) + self.layer_biases[-1]\n",
    "\n",
    "        final_output = softmax(final_input)\n",
    "        activations.append(final_output)\n",
    "        return final_output, activations\n",
    "\n",
    "    def backward_propagation(self, x, y):\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.layer_weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.layer_biases]\n",
    "        final_output, activations = self.propagate_forward(x)\n",
    "        zs = [\n",
    "            np.dot(w, act) + b\n",
    "            for w, b, act in zip(self.layer_weights, self.layer_biases, activations[:-1])\n",
    "        ]\n",
    "\n",
    "        # Cross-entropy and softmax derivative\n",
    "        delta = cross_entropy_loss_derivative(final_output, y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "        # Propagate the error backwards\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            sp = relu_derivative(z)\n",
    "            delta = np.dot(self.layer_weights[-l + 1].T, delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "\n",
    "        return nabla_w, nabla_b\n",
    "\n",
    "    \n",
    "    # def backprop(self, x, y):\n",
    "    #     nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    #     nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    #     final_output, activations = self.feedforward(x)\n",
    "    #     zs = [\n",
    "    #         np.dot(w, act) + b\n",
    "    #         for w, b, act in zip(self.weights, self.biases, activations[:-1])\n",
    "    #     ]\n",
    "    # \n",
    "    #     # Cross-entropy and softmax derivative\n",
    "    #     delta = cross_entropy_derivative(final_output, y)\n",
    "    #     nabla_b[-1] = delta\n",
    "    #     nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
    "    # \n",
    "    #     # Propagate the error backwards\n",
    "    #     for l in range(2, len(self.layer_sizes)):\n",
    "    #         z = zs[-l]\n",
    "    #         sp = relu_derivative(z)\n",
    "    #         delta = np.dot(self.weights[-l + 1].T, delta) * sp\n",
    "    #         nabla_b[-l] = delta\n",
    "    #         nabla_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "    # \n",
    "    #     return nabla_w, nabla_b\n",
    "    \n",
    "    def update_batch(self, batch, learn_rate, regularization, total_size, optimization_method, beta, epsilon=None):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of the network for a single batch using gradient descent with regularization\n",
    "        and optionally applies momentum or RMSprop as optimization methods to accelerate convergence.\n",
    "    \n",
    "        Parameters:\n",
    "        - batch (list of tuples): Each tuple contains input data and true labels/values for a batch of samples.\n",
    "        - learn_rate (float): Learning rate for the optimization.\n",
    "        - regularization (float): Regularization factor to reduce overfitting by penalizing large weights.\n",
    "        - total_size (int): Total number of samples in the dataset, used for regularization calculation.\n",
    "        - optimization_method (str): Specifies the optimization method ('momentum' or 'rmsprop').\n",
    "        - beta (float): Hyperparameter for the optimization methods, affecting the weighting of past gradients.\n",
    "        - epsilon (float, optional): A small number to avoid division by zero in 'rmsprop', default is None.\n",
    "    \n",
    "        Returns:\n",
    "        - None, but updates the network's weights and biases in place.\n",
    "        \"\"\"\n",
    "        # Initialize gradients for weights and biases with zeros\n",
    "        gradient_w = [np.zeros(weight.shape) for weight in self.layer_weights]\n",
    "        gradient_b = [np.zeros(bias.shape) for bias in self.layer_biases]\n",
    "        \n",
    "        # Loop through each data point in the batch\n",
    "        for input_val, true_val in batch:\n",
    "            # Perform backpropagation to compute gradients for current data point\n",
    "            delta_gradient_w, delta_gradient_b = self.backward_propagation(input_val, true_val)\n",
    "            # Accumulate gradients over the batch\n",
    "            gradient_w = [w + dw for w, dw in zip(gradient_w, delta_gradient_w)]\n",
    "            gradient_b = [b + db for b, db in zip(gradient_b, delta_gradient_b)]\n",
    "\n",
    "        # Apply optimization method if specified\n",
    "        if optimization_method == 'momentum':\n",
    "            # Initialize velocity terms for weights and biases if not already initialized\n",
    "            if not hasattr(self, 'velocity_weights'):\n",
    "                self.velocity_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.velocity_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "                \n",
    "            # Update velocities based on gradients and apply to weights and biases\n",
    "            self.velocity_weights = [beta * vw + (1 - beta) * gw for vw, gw in zip(self.velocity_weights, gradient_w)]\n",
    "            self.velocity_biases = [beta * vb + (1 - beta) * gb for vb, gb in zip(self.velocity_biases, gradient_b)]\n",
    "            # Update weights and biases using momentum method\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - learn_rate * vw\n",
    "                                  for w, vw in zip(self.layer_weights, self.velocity_weights)]\n",
    "            self.layer_biases = [b - learn_rate * vb for b, vb in zip(self.layer_biases, self.velocity_biases)]\n",
    "            \n",
    "        elif optimization_method == 'rmsprop':\n",
    "            # Initialize squared gradient terms for weights and biases if not already initialized\n",
    "            if not hasattr(self, 'squared_gradients_weights'):\n",
    "                self.squared_gradients_weights = [np.zeros_like(w) for w in self.layer_weights]\n",
    "                self.squared_gradients_biases = [np.zeros_like(b) for b in self.layer_biases]\n",
    "                \n",
    "            # Update squared gradients based on new gradients\n",
    "            self.squared_gradients_weights = [beta * sgw + (1 - beta) * (gw**2)\n",
    "                                              for sgw, gw in zip(self.squared_gradients_weights, gradient_w)]\n",
    "            self.squared_gradients_biases = [beta * sgb + (1 - beta) * (gb**2)\n",
    "                                             for sgb, gb in zip(self.squared_gradients_biases, gradient_b)]\n",
    "            # Update weights and biases using rmsprop method\n",
    "            self.layer_weights = [(1 - learn_rate * (regularization / total_size)) * w - \n",
    "                                  (learn_rate / (np.sqrt(sgw) + epsilon)) * gw\n",
    "                                  for w, sgw, gw in zip(self.layer_weights, self.squared_gradients_weights, gradient_w)]\n",
    "            self.layer_biases = [b - (learn_rate / (np.sqrt(sgb) + epsilon)) * gb\n",
    "                                 for b, sgb, gb in zip(self.layer_biases, self.squared_gradients_biases, gradient_b)]\n",
    "    \n",
    "    def train(self, training_set, epoch_count, learn_rate, batch_size_input, regularization=0.0,\n",
    "              visual_interval=100, optimization_method='momentum', beta=0.9, epsilon=1e-8, \n",
    "              X_val_scaled=None, y_val_scaled=None, X_val=None, y_val=None,\n",
    "              scaler_X=None, scaler_y=None, mse_limit=None):\n",
    "        \"\"\"\n",
    "        Trains the neural network over a specified number of epochs using mini-batch gradient descent,\n",
    "        and tracks performance over epochs through mean squared error on a validation set. Supports learning\n",
    "        rate scheduling, regularization, and early stopping.\n",
    "    \n",
    "        Parameters:\n",
    "        - training_set (list): Training data consisting of tuples of input values and true labels.\n",
    "        - epoch_count (int): Total number of epochs to train the network.\n",
    "        - learn_rate (float): Initial learning rate for optimization.\n",
    "        - batch_size_input (int or float): Size of the mini-batches for training. If float, it represents\n",
    "          the percentage of the total dataset size.\n",
    "        - regularization (float): Regularization factor for reducing overfitting.\n",
    "        - visual_interval (int): Frequency of epochs at which to calculate and print the MSE for tracking.\n",
    "        - optimization_method (str): Optimization method to use ('momentum' or 'rmsprop').\n",
    "        - beta (float): Parameter for the optimization method that controls the momentum or the weighted average.\n",
    "        - epsilon (float): Small value to prevent division by zero in 'rmsprop'.\n",
    "        - X_val_scaled, y_val_scaled, X_val, y_val (ndarray, optional): Validation datasets for performance evaluation.\n",
    "        - scaler_X, scaler_y (preprocessing scaler objects, optional): Scalers used for transforming data back to original scale for MSE calculation.\n",
    "        - mse_limit (float, optional): MSE value at which training can be stopped early for performance.\n",
    "    \n",
    "        Returns:\n",
    "        - mse_history (list): History of mean squared error values at each visual interval.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Initialize an empty list to store the history of mean squared errors (MSE) for each epoch\n",
    "        loss_history = []\n",
    "        # Determine the total number of samples in the training set\n",
    "        data_size = len(training_set)\n",
    "        # Store the initial learning rate, which may be adjusted during training\n",
    "        rate_init = learn_rate\n",
    "    \n",
    "        # Determine mini-batch size based on whether the batch_size_input is a percentage or fixed value\n",
    "        if isinstance(batch_size_input, float):  # If batch_size_input is a float, treat it as a percentage\n",
    "            mini_batch_size = max(1, min(data_size, int(data_size * batch_size_input / 100)))\n",
    "        elif isinstance(batch_size_input, int):  # If batch_size_input is an integer, treat it as a fixed size\n",
    "            mini_batch_size = max(1, min(data_size, batch_size_input))\n",
    "        else:  # Raise an error if batch_size_input is neither float nor int\n",
    "            raise ValueError(\"batch_size_input must be an integer (fixed size) or a float (percentage of dataset)\")\n",
    "    \n",
    "        # Iterate through each epoch for training\n",
    "        for epoch in range(epoch_count):\n",
    "            # Randomly shuffle the training set to ensure randomness of mini-batches\n",
    "            np.random.shuffle(training_set)\n",
    "            # Create mini-batches from the training set\n",
    "            mini_batches = [training_set[k:k + mini_batch_size] for k in range(0, data_size, mini_batch_size)]\n",
    "            # Update the model's weights and biases for each mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_batch(mini_batch, learn_rate, regularization, data_size, optimization_method, beta, epsilon)\n",
    "    \n",
    "            # Adjust the learning rate after each epoch (simple learning rate decay)\n",
    "            learn_rate = rate_init / (1 + 0.01 * epoch)\n",
    "    \n",
    "            # At specified intervals or the last epoch, evaluate and print the model's performance\n",
    "            if epoch % visual_interval == 0 or epoch == epoch_count - 1:\n",
    "                '''\n",
    "                # Generate predictions for the validation set\n",
    "                predictions = np.array([self.propagate_forward(x.reshape(-1, 1))[0] for x in X_val])\n",
    "\n",
    "                # Calculate the cross-entropy loss on the validation set\n",
    "                # Assuming y_val is one-hot encoded and predictions are the output of the softmax layer\n",
    "                cross_entropy_loss_val = cross_entropy_loss(predictions, y_val)\n",
    "\n",
    "                # Replace mse_history with a more appropriately named variable, like loss_history\n",
    "                loss_history.append((epoch, cross_entropy_loss_val))\n",
    "\n",
    "                # Print the current epoch and its cross-entropy loss\n",
    "                print(f'Epoch {epoch}, Cross-Entropy Loss: {cross_entropy_loss_val}')\n",
    "\n",
    "\n",
    "                # If a threshold for MSE is set and the current MSE is below this threshold, stop training\n",
    "                if mse_limit is not None and cross_entropy_loss_val < mse_limit:\n",
    "                    break \n",
    "                '''\n",
    "                predictions = np.argmax(np.array([self.propagate_forward(x.reshape(-1, 1))[0] for x in X_val]), axis=1)\n",
    "                # Calculate accuracy or other metrics\n",
    "                accuracy = np.mean(predictions == y_val)\n",
    "                print(f'epoch: {epoch}', f'Test accuracy: {accuracy}')\n",
    "                \n",
    "                # Calculate F1 Score\n",
    "                f1_weighted = f1_score(y_val, predictions, average='weighted')\n",
    "                print(f\"F1 Score (Weighted): {f1_weighted}\")\n",
    "        # \n",
    "        # # Return the history of MSE values\n",
    "        # return loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:19.592875Z",
     "start_time": "2024-03-25T23:14:19.589402Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataScaler:\n",
    "    def __init__(self, method=\"standardization\"):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.fit_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.fit_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        if self.method == \"min_max\":\n",
    "            return self.inverse_transform_min_max(data)\n",
    "        elif self.method == \"standardization\":\n",
    "            return self.inverse_transform_standardization(data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "    def fit_transform_min_max(self, data):\n",
    "        self.min = np.min(data, axis=0)\n",
    "        self.max = np.max(data, axis=0)\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def transform_min_max(self, data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse_transform_min_max(self, data):\n",
    "        return data * (self.max - self.min) + self.min\n",
    "\n",
    "    def fit_transform_standardization(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def transform_standardization(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform_standardization(self, data):\n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:19.778183Z",
     "start_time": "2024-03-25T23:14:19.776227Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse(mse_history):\n",
    "    plt.plot(mse_history)\n",
    "    plt.title('MSE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:20.067177Z",
     "start_time": "2024-03-25T23:14:20.062747Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_rings3_regular = pd.read_csv('./data/classification/rings3-regular-training.csv')\n",
    "df_test_rings3_regular = pd.read_csv('./data/classification/rings3-regular-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:20.227372Z",
     "start_time": "2024-03-25T23:14:20.223377Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_easy = pd.read_csv('./data/classification/easy-training.csv')\n",
    "df_test_easy = pd.read_csv('./data/classification/easy-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:20.380401Z",
     "start_time": "2024-03-25T23:14:20.377003Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_xor3 = pd.read_csv('./data/classification/xor3-training.csv')\n",
    "df_test_xor3 = pd.read_csv('./data/classification/xor3-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rings 3 regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:20.679908Z",
     "start_time": "2024-03-25T23:14:20.678254Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler_X = DataScaler(\"standardization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:20.826891Z",
     "start_time": "2024-03-25T23:14:20.824551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_rings = df_train_rings3_regular[['x']].values.reshape(-1, 1)\n",
    "X1_test_rings = df_test_rings3_regular[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:20.987718Z",
     "start_time": "2024-03-25T23:14:20.985155Z"
    }
   },
   "outputs": [],
   "source": [
    "X2_train_rings = df_train_rings3_regular[['y']].values.reshape(-1, 1)\n",
    "X2_test_rings = df_test_rings3_regular[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:21.128877Z",
     "start_time": "2024-03-25T23:14:21.126676Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rings = np.hstack((X1_train_rings, X2_train_rings))\n",
    "X_test_rings = np.hstack((X1_test_rings, X2_test_rings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:21.289038Z",
     "start_time": "2024-03-25T23:14:21.286890Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rings_scaled = np.hstack((scaler_X.fit_transform(X1_train_rings), scaler_X.fit_transform(X2_train_rings)))\n",
    "X_test_rings_scaled = np.hstack((scaler_X.transform(X1_test_rings), scaler_X.transform(X2_test_rings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:21.450313Z",
     "start_time": "2024-03-25T23:14:21.448274Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_rings = df_train_rings3_regular['c'].values.reshape(-1, 1)\n",
    "y_test_rings = df_test_rings3_regular['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:21.615583Z",
     "start_time": "2024-03-25T23:14:21.612718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_rings = encoder.fit_transform(y_train_rings)\n",
    "y_test_encoded_rings = encoder.transform(y_test_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:21.760368Z",
     "start_time": "2024-03-25T23:14:21.758127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_rings = y_train_encoded_rings.shape[1] \n",
    "num_classes_rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:21.931823Z",
     "start_time": "2024-03-25T23:14:21.928965Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_rings = [\n",
    "    (X_train_rings[i].reshape(-1, 1), y_train_encoded_rings[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_rings))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:33.959413Z",
     "start_time": "2024-03-25T23:16:08.336451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0 Test accuracy: 0.35933333333333334\n",
      "F1 Score (Weighted): 0.29853551756812857\n",
      "epoch: 10 Test accuracy: 0.38866666666666666\n",
      "F1 Score (Weighted): 0.3617061569349186\n",
      "epoch: 20 Test accuracy: 0.3253333333333333\n",
      "F1 Score (Weighted): 0.20551136460518016\n",
      "epoch: 30 Test accuracy: 0.33266666666666667\n",
      "F1 Score (Weighted): 0.16641654160413538\n",
      "epoch: 40 Test accuracy: 0.338\n",
      "F1 Score (Weighted): 0.3344151571654099\n",
      "epoch: 50 Test accuracy: 0.3506666666666667\n",
      "F1 Score (Weighted): 0.34737665608273466\n",
      "epoch: 60 Test accuracy: 0.368\n",
      "F1 Score (Weighted): 0.24817357432801213\n",
      "epoch: 70 Test accuracy: 0.3526666666666667\n",
      "F1 Score (Weighted): 0.30375579925435864\n",
      "epoch: 80 Test accuracy: 0.37066666666666664\n",
      "F1 Score (Weighted): 0.3247667064326778\n",
      "epoch: 90 Test accuracy: 0.38533333333333336\n",
      "F1 Score (Weighted): 0.2995829165231746\n",
      "epoch: 100 Test accuracy: 0.31866666666666665\n",
      "F1 Score (Weighted): 0.3167382498489869\n",
      "epoch: 110 Test accuracy: 0.368\n",
      "F1 Score (Weighted): 0.29494929458090263\n",
      "epoch: 120 Test accuracy: 0.326\n",
      "F1 Score (Weighted): 0.2921249058425956\n",
      "epoch: 130 Test accuracy: 0.43066666666666664\n",
      "F1 Score (Weighted): 0.41337816159156476\n",
      "epoch: 140 Test accuracy: 0.33666666666666667\n",
      "F1 Score (Weighted): 0.3262320186962445\n",
      "epoch: 150 Test accuracy: 0.338\n",
      "F1 Score (Weighted): 0.2705925408203631\n",
      "epoch: 160 Test accuracy: 0.21133333333333335\n",
      "F1 Score (Weighted): 0.21330913487173422\n",
      "epoch: 170 Test accuracy: 0.354\n",
      "F1 Score (Weighted): 0.33572921591081395\n",
      "epoch: 180 Test accuracy: 0.3293333333333333\n",
      "F1 Score (Weighted): 0.3191128868567055\n",
      "epoch: 190 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 200 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 210 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 220 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 230 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 240 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 250 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 260 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 270 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 280 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 290 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 300 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 310 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 320 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 330 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 340 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 350 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 360 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 370 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 380 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 390 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 400 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 410 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 420 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 430 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 440 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 450 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 460 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 470 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 480 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 490 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 500 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 510 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 520 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 530 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 540 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 550 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 560 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 570 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 580 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 590 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 600 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 610 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 620 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 630 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 640 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 650 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 660 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 670 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 680 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 690 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 700 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 710 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 720 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 730 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 740 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 750 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 760 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 770 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 780 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 790 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 800 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 810 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 820 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 830 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 840 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 850 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 860 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 870 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 880 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 890 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 900 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 910 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 920 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 930 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 940 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 950 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 960 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 970 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 980 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 990 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n",
      "epoch: 999 Test accuracy: 0.35533333333333333\n",
      "F1 Score (Weighted): 0.21073860028811855\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_rings = MLP(sizes=[2, 10, 3])  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_rings.train(training_set=training_data_rings, epoch_count=1000, learn_rate=0.01, batch_size_input=32, X_val=X_train_rings, y_val=y_train_rings, visual_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:50.311327Z",
     "start_time": "2024-03-25T23:14:50.277657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_rings = np.argmax(np.array([mlp_rings.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_rings]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:51.165878Z",
     "start_time": "2024-03-25T23:14:51.163488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.208\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy or other metrics\n",
    "accuracy_rings = np.mean(predictions_rings == y_test_rings)\n",
    "print(f'Test accuracy: {accuracy_rings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:52.193543Z",
     "start_time": "2024-03-25T23:14:52.188989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.07162913907284768\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted = f1_score(y_test_rings, predictions_rings, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:14:57.137499Z",
     "start_time": "2024-03-25T23:14:57.133343Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_rings_scaled = [\n",
    "    (X_train_rings_scaled[i].reshape(-1, 1), y_train_encoded_rings[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_rings_scaled))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:15:56.529623Z",
     "start_time": "2024-03-25T23:15:30.314321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0 Test accuracy: 0.288\n",
      "F1 Score (Weighted): 0.2335040519972027\n",
      "epoch: 10 Test accuracy: 0.25866666666666666\n",
      "F1 Score (Weighted): 0.20783827807422253\n",
      "epoch: 20 Test accuracy: 0.2693333333333333\n",
      "F1 Score (Weighted): 0.2146007797411972\n",
      "epoch: 30 Test accuracy: 0.2806666666666667\n",
      "F1 Score (Weighted): 0.22477769671857845\n",
      "epoch: 40 Test accuracy: 0.2713333333333333\n",
      "F1 Score (Weighted): 0.2144083744287592\n",
      "epoch: 50 Test accuracy: 0.274\n",
      "F1 Score (Weighted): 0.2160125943822753\n",
      "epoch: 60 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21419249368062424\n",
      "epoch: 70 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.214329552081132\n",
      "epoch: 80 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.213182803983401\n",
      "epoch: 90 Test accuracy: 0.2833333333333333\n",
      "F1 Score (Weighted): 0.21877573670122508\n",
      "epoch: 100 Test accuracy: 0.284\n",
      "F1 Score (Weighted): 0.2158619011359087\n",
      "epoch: 110 Test accuracy: 0.2826666666666667\n",
      "F1 Score (Weighted): 0.21685210488645976\n",
      "epoch: 120 Test accuracy: 0.278\n",
      "F1 Score (Weighted): 0.21423695675989987\n",
      "epoch: 130 Test accuracy: 0.2813333333333333\n",
      "F1 Score (Weighted): 0.2163543632897052\n",
      "epoch: 140 Test accuracy: 0.2813333333333333\n",
      "F1 Score (Weighted): 0.2163543632897052\n",
      "epoch: 150 Test accuracy: 0.2786666666666667\n",
      "F1 Score (Weighted): 0.214637950798428\n",
      "epoch: 160 Test accuracy: 0.28\n",
      "F1 Score (Weighted): 0.21555568815979617\n",
      "epoch: 170 Test accuracy: 0.2793333333333333\n",
      "F1 Score (Weighted): 0.2150000692108578\n",
      "epoch: 180 Test accuracy: 0.2786666666666667\n",
      "F1 Score (Weighted): 0.21581144781144782\n",
      "epoch: 190 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.21395081344213865\n",
      "epoch: 200 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.2141823581475477\n",
      "epoch: 210 Test accuracy: 0.2773333333333333\n",
      "F1 Score (Weighted): 0.21387500950248448\n",
      "epoch: 220 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.2141312103895478\n",
      "epoch: 230 Test accuracy: 0.2793333333333333\n",
      "F1 Score (Weighted): 0.21644375844039218\n",
      "epoch: 240 Test accuracy: 0.278\n",
      "F1 Score (Weighted): 0.21544707041884636\n",
      "epoch: 250 Test accuracy: 0.2813333333333333\n",
      "F1 Score (Weighted): 0.2177778381341433\n",
      "epoch: 260 Test accuracy: 0.2793333333333333\n",
      "F1 Score (Weighted): 0.21625596568181193\n",
      "epoch: 270 Test accuracy: 0.2806666666666667\n",
      "F1 Score (Weighted): 0.21770602871341038\n",
      "epoch: 280 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.21476066762410373\n",
      "epoch: 290 Test accuracy: 0.278\n",
      "F1 Score (Weighted): 0.21544707041884636\n",
      "epoch: 300 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21275017253278125\n",
      "epoch: 310 Test accuracy: 0.2786666666666667\n",
      "F1 Score (Weighted): 0.21607825174429107\n",
      "epoch: 320 Test accuracy: 0.278\n",
      "F1 Score (Weighted): 0.21544707041884636\n",
      "epoch: 330 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21310509047755705\n",
      "epoch: 340 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21382050949085157\n",
      "epoch: 350 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21369715678898396\n",
      "epoch: 360 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21346666916714466\n",
      "epoch: 370 Test accuracy: 0.2773333333333333\n",
      "F1 Score (Weighted): 0.21407198426947077\n",
      "epoch: 380 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2127908603997303\n",
      "epoch: 390 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.21347354022398907\n",
      "epoch: 400 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2127908603997303\n",
      "epoch: 410 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.2138690096142878\n",
      "epoch: 420 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21310509047755705\n",
      "epoch: 430 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2127908603997303\n",
      "epoch: 440 Test accuracy: 0.2786666666666667\n",
      "F1 Score (Weighted): 0.21634229956614032\n",
      "epoch: 450 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.2153713938886114\n",
      "epoch: 460 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21400653646692788\n",
      "epoch: 470 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21400653646692788\n",
      "epoch: 480 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.2130132942840239\n",
      "epoch: 490 Test accuracy: 0.274\n",
      "F1 Score (Weighted): 0.21335278230212565\n",
      "epoch: 500 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2149010015181019\n",
      "epoch: 510 Test accuracy: 0.27266666666666667\n",
      "F1 Score (Weighted): 0.2123671497584541\n",
      "epoch: 520 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21543516832723442\n",
      "epoch: 530 Test accuracy: 0.2773333333333333\n",
      "F1 Score (Weighted): 0.2170098287662716\n",
      "epoch: 540 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21414925451316275\n",
      "epoch: 550 Test accuracy: 0.2733333333333333\n",
      "F1 Score (Weighted): 0.21299084153893746\n",
      "epoch: 560 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21419249368062424\n",
      "epoch: 570 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.214020397208803\n",
      "epoch: 580 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21460010735373056\n",
      "epoch: 590 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.214368100181249\n",
      "epoch: 600 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21423599776188262\n",
      "epoch: 610 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21419249368062424\n",
      "epoch: 620 Test accuracy: 0.274\n",
      "F1 Score (Weighted): 0.2135700480260501\n",
      "epoch: 630 Test accuracy: 0.274\n",
      "F1 Score (Weighted): 0.21391626352848506\n",
      "epoch: 640 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.2147904022667651\n",
      "epoch: 650 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.214368100181249\n",
      "epoch: 660 Test accuracy: 0.274\n",
      "F1 Score (Weighted): 0.21365779596945705\n",
      "epoch: 670 Test accuracy: 0.2773333333333333\n",
      "F1 Score (Weighted): 0.2172134617136305\n",
      "epoch: 680 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21598128791566956\n",
      "epoch: 690 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.214368100181249\n",
      "epoch: 700 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.216392562445095\n",
      "epoch: 710 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2149010015181019\n",
      "epoch: 720 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21414925451316275\n",
      "epoch: 730 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21560890481301592\n",
      "epoch: 740 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21485746757622345\n",
      "epoch: 750 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21485746757622345\n",
      "epoch: 760 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.21453642858552396\n",
      "epoch: 770 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21547820209530247\n",
      "epoch: 780 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2149010015181019\n",
      "epoch: 790 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21485746757622345\n",
      "epoch: 800 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.214368100181249\n",
      "epoch: 810 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21556507026735508\n",
      "epoch: 820 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.2161846100519336\n",
      "epoch: 830 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2149010015181019\n",
      "epoch: 840 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2149010015181019\n",
      "epoch: 850 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2152770629610568\n",
      "epoch: 860 Test accuracy: 0.2773333333333333\n",
      "F1 Score (Weighted): 0.2170098287662716\n",
      "epoch: 870 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.216392562445095\n",
      "epoch: 880 Test accuracy: 0.27466666666666667\n",
      "F1 Score (Weighted): 0.2147023503173786\n",
      "epoch: 890 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.2149010015181019\n",
      "epoch: 900 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.216392562445095\n",
      "epoch: 910 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.21655451806757556\n",
      "epoch: 920 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21536412573205485\n",
      "epoch: 930 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.216392562445095\n",
      "epoch: 940 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.21655451806757556\n",
      "epoch: 950 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21560890481301592\n",
      "epoch: 960 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21498886744396126\n",
      "epoch: 970 Test accuracy: 0.2753333333333333\n",
      "F1 Score (Weighted): 0.21515586499285708\n",
      "epoch: 980 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21593784919734457\n",
      "epoch: 990 Test accuracy: 0.27666666666666667\n",
      "F1 Score (Weighted): 0.21655451806757556\n",
      "epoch: 999 Test accuracy: 0.276\n",
      "F1 Score (Weighted): 0.21593784919734457\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_rings = MLP(sizes=[2, 10, 3])  # Example layer setup\n",
    "\n",
    "print()\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_rings.train(training_set=training_data_rings_scaled, epoch_count=1000, learn_rate=0.01, batch_size_input=20, X_val=X_test_rings, y_val=y_train_rings, visual_interval=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.637752Z",
     "start_time": "2024-03-25T21:54:49.618892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_rings = np.argmax(np.array([mlp_rings.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_rings]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.642295Z",
     "start_time": "2024-03-25T21:54:49.638805Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.421\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy or other metrics\n",
    "accuracy_rings = np.mean(predictions_rings == y_test_rings)\n",
    "print(f'Test accuracy: {accuracy_rings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T21:54:49.647057Z",
     "start_time": "2024-03-25T21:54:49.644106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.35879514074384916\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted = f1_score(y_test_rings, predictions_rings, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### easy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:39.135905Z",
     "start_time": "2024-03-25T23:16:39.132125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X1_train_easy = df_train_easy[['x']].values.reshape(-1, 1)\n",
    "X1_test_easy = df_test_easy[['x']].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:39.780051Z",
     "start_time": "2024-03-25T23:16:39.777845Z"
    }
   },
   "outputs": [],
   "source": [
    "X2_train_easy = df_train_easy[['y']].values.reshape(-1, 1)\n",
    "X2_test_easy = df_test_easy[['y']].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:40.074851Z",
     "start_time": "2024-03-25T23:16:40.067807Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_easy= np.hstack((X1_train_easy, X2_train_easy))\n",
    "X_test_easy = np.hstack((X1_test_easy, X2_test_easy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:40.320367Z",
     "start_time": "2024-03-25T23:16:40.318023Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_easy = df_train_easy['c'].values.reshape(-1, 1)\n",
    "y_test_easy = df_test_easy['c'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:40.573987Z",
     "start_time": "2024-03-25T23:16:40.571059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_easy = encoder.fit_transform(y_train_easy)\n",
    "y_test_encoded_easy = encoder.transform(y_test_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:40.849465Z",
     "start_time": "2024-03-25T23:16:40.846837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_easy = y_train_encoded_easy.shape[1] \n",
    "num_classes_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:16:41.862344Z",
     "start_time": "2024-03-25T23:16:41.859482Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_easy = [\n",
    "    (X_train_easy[i].reshape(-1, 1), y_train_encoded_easy[i].reshape(-1, 1))\n",
    "    for i in range(len(X_train_easy))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:19:56.730964Z",
     "start_time": "2024-03-25T23:19:08.096144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939817833442004\n",
      "epoch: 100 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 200 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 300 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 400 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 500 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 600 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 700 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 800 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 900 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 1000 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 1100 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 1200 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 1300 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 1400 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 1500 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 1600 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 1700 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 1800 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 1900 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 2000 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 2100 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 2200 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 2300 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 2400 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 2500 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 2600 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 2700 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 2800 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 2900 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 3000 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 3100 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 3200 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 3300 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 3400 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 3500 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 3600 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 3700 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 3800 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 3900 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4000 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4100 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4200 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4300 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 4400 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 4500 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 4600 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n",
      "epoch: 4700 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4800 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4900 Test accuracy: 0.48\n",
      "F1 Score (Weighted): 0.47992510921572706\n",
      "epoch: 4999 Test accuracy: 0.494\n",
      "F1 Score (Weighted): 0.4939493949394939\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "# num_classes_rings = y_train_encoded_rings.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_easy = MLP(sizes=[2, 5, 2])  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "mlp_easy.train(training_set=training_data_easy, epoch_count=5000, learn_rate=0.01, batch_size_input=5, X_val=X_test_easy, y_val=y_train_easy, visual_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.390545Z",
     "start_time": "2024-03-25T22:09:21.377572Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_easy = np.argmax(np.array([mlp_easy.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_easy]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.393675Z",
     "start_time": "2024-03-25T22:09:21.391329Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_easy = np.mean(predictions_easy == y_test_easy)\n",
    "print(f'Test accuracy: {accuracy_easy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T22:09:21.398427Z",
     "start_time": "2024-03-25T22:09:21.394888Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.8032047129104457\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_weighted = f1_score(y_test_easy, predictions_easy, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:15.898651Z",
     "start_time": "2024-03-25T10:00:15.895604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_X = DataScaler(\"standardization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.028622Z",
     "start_time": "2024-03-25T10:00:16.026353Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_train_xor = df_train_xor3[['x', 'y']].values\n",
    "X_test_xor = df_test_xor3[['x', 'y']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.177906Z",
     "start_time": "2024-03-25T10:00:16.175836Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use your custom scaler to scale the X values\n",
    "X_train_scaled_xor = scaler_X.fit_transform(X_train_xor)\n",
    "X_test_scaled_xor = scaler_X.transform(X_test_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.319675Z",
     "start_time": "2024-03-25T10:00:16.315479Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode the 'c' column into one-hot vectors for the training and test datasets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded_xor = encoder.fit_transform(df_train_xor3[['c']])\n",
    "y_test_encoded_xor = encoder.transform(df_test_xor3[['c']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.456767Z",
     "start_time": "2024-03-25T10:00:16.454549Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the MLP with the proper layer sizes.\n",
    "# For example, with 2 input features, a hidden layer with 10 neurons, and output layer matching the number of classes\n",
    "num_classes_xor = y_train_encoded_xor.shape[1]  # Assuming y_train_encoded is one-hot encoded\n",
    "mlp_xor = MLP(sizes=[2, 10, num_classes_xor])  # Example layer setup\n",
    "\n",
    "# Train the MLP using your training data\n",
    "# This step will vary depending on the exact implementation of your `train` method\n",
    "# For example:\n",
    "# mlp.train(training_set=list(zip(X_train_scaled, y_train_encoded)), epoch_count=100, learn_rate=0.01, batch_size_input=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.601251Z",
     "start_time": "2024-03-25T10:00:16.590068Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "# This might involve looping through X_test_scaled and using your model's predict method\n",
    "predictions_xor = np.argmax(np.array([mlp_xor.propagate_forward(x.reshape(-1, 1))[0] for x in X_test_scaled_xor]), axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test_encoded back to labels for comparison\n",
    "y_test_labels_xor = np.argmax(y_test_encoded_xor, axis=1)\n",
    "\n",
    "# Calculate accuracy or other metrics\n",
    "accuracy_xor = np.mean(predictions_xor == y_test_labels_xor)\n",
    "print(f'Test accuracy: {accuracy_xor}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T10:00:16.840031Z",
     "start_time": "2024-03-25T10:00:16.834164Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Macro): 0.3702770780856423\n",
      "F1 Score (Micro): 0.588\n",
      "F1 Score (Weighted): 0.4354458438287153\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_test_labels_xor, predictions_xor, average='macro')\n",
    "f1_micro = f1_score(y_test_labels_xor, predictions_xor, average='micro')\n",
    "f1_weighted = f1_score(y_test_labels_xor, predictions_xor, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
